{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "38bf86bc-1bef-4b68-8f6a-c08a6d91202d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/07/08 18:17:37 WARN Utils: Your hostname, Pavels-MacBook-Air.local resolves to a loopback address: 127.0.0.200; using 192.168.0.103 instead (on interface en0)\n",
      "23/07/08 18:17:37 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "23/07/08 18:17:37 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://192.168.0.103:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.1.2</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>app</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7fe918980eb0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '/Users/pyro/github/HiveHelper_on_PySpark/hhop') \n",
    "# for running .ipynb files anywhere outside of a current dir using the module hhop\n",
    "\n",
    "from functools import reduce\n",
    "from importlib import reload\n",
    "import pandas as pd\n",
    "\n",
    "from pyspark.sql.functions import col\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.window import Window as W\n",
    "import pyspark.sql.types as T\n",
    "\n",
    "import shutil, os, time # working with FS\n",
    "from glob import glob\n",
    "from shutil import copy2\n",
    "from pathlib import Path\n",
    "\n",
    "import hhop # custom module\n",
    "from hhop import DFExtender, SchemaManager, TablePartitionDescriber #main classes\n",
    "import funs\n",
    "from funs import read_table, write_table, union_all, deduplicate_df # useful functions\n",
    "from spark_init import spark\n",
    "from exceptions import HhopException\n",
    "display(spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dad81453-9c1c-4d6b-acdf-66f8844858c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "s1 = spark.read.csv('scd2_data/to_scd2.csv', sep=';', header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ed2e6531-2c80-4690-888e-61eb292d3b0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+------+------+------+-----------+-------------------+\n",
      "|pk1|pk2 |nonpk1|nonpk2|nonpk3|nonpk_extra|ts                 |\n",
      "+---+----+------+------+------+-----------+-------------------+\n",
      "|v1 |c1  |a1    |b1    |c1    |r1         |2023-05-01 10:00:00|\n",
      "|v1 |c1  |a1    |b1    |c1    |r2         |2023-05-01 12:00:00|\n",
      "|v1 |c1  |a1    |b1    |c1    |r3         |2023-05-02 12:00:00|\n",
      "|v1 |c1  |a1    |b1    |c2    |null       |2023-05-03 12:00:00|\n",
      "|v1 |c1  |a1    |b2    |c2    |null       |2023-05-03 15:00:00|\n",
      "|v1 |c1  |null  |b2    |c2    |r3         |2023-05-05 15:00:00|\n",
      "|v1 |c1  |null  |b2    |c2    |r3         |2023-05-06 15:00:00|\n",
      "|v1 |c1  |null  |null  |c2    |r3         |2023-05-07 15:00:00|\n",
      "|v1 |c1  |null  |null  |null  |null       |2023-05-10 15:00:00|\n",
      "|v1 |c1  |null  |null  |c2    |r3         |2023-05-13 15:00:00|\n",
      "|v1 |null|null  |null  |c2    |r3         |2023-05-07 15:00:00|\n",
      "|v1 |null|null  |null  |null  |null       |2023-05-10 15:00:00|\n",
      "|v1 |null|null  |fds   |null  |null       |2023-05-11 15:00:00|\n",
      "|v1 |null|null  |fds   |asdf  |null       |2023-05-12 15:00:00|\n",
      "+---+----+------+------+------+-----------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "s1.show(100, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5574f6a4-cff8-410b-b68f-f36d1a8a9ec2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+------+------+------+-----------+-------------------+--------------------------------+---------------+-------------+\n",
      "|pk1|pk2 |nonpk1|nonpk2|nonpk3|nonpk_extra|ts                 |row_hash                        |row_actual_from|row_actual_to|\n",
      "+---+----+------+------+------+-----------+-------------------+--------------------------------+---------------+-------------+\n",
      "|v1 |null|null  |null  |c2    |r3         |2023-05-07 15:00:00|56e6807f4b745e20dffeb1b731e5a6d4|2023-05-07     |2023-05-09   |\n",
      "|v1 |null|null  |null  |null  |null       |2023-05-10 15:00:00|6654c734ccab8f440ff0825eb443dc7f|2023-05-10     |2023-05-10   |\n",
      "|v1 |null|null  |fds   |null  |null       |2023-05-11 15:00:00|2d2722576095dd7996570b307d777539|2023-05-11     |2023-05-11   |\n",
      "|v1 |null|null  |fds   |asdf  |null       |2023-05-12 15:00:00|b08363345cd7c1cb14e6f4747ce1563d|2023-05-12     |9999-12-31   |\n",
      "|v1 |c1  |a1    |b1    |c1    |r1         |2023-05-01 10:00:00|93e6cc4b8b0445cf261e9417106ae6f0|2023-05-01     |2023-05-02   |\n",
      "|v1 |c1  |a1    |b2    |c2    |null       |2023-05-03 15:00:00|a6244d3c7c2aed33c4d9525fbef29c1d|2023-05-03     |2023-05-04   |\n",
      "|v1 |c1  |null  |b2    |c2    |r3         |2023-05-05 15:00:00|17f599be9e07976c2036361c9ad8f633|2023-05-05     |2023-05-06   |\n",
      "|v1 |c1  |null  |null  |c2    |r3         |2023-05-07 15:00:00|a363a9dd6d5b30865ab5813581941516|2023-05-07     |2023-05-09   |\n",
      "|v1 |c1  |null  |null  |null  |null       |2023-05-10 15:00:00|da58ea33b20d82042d9969c46c16c3b8|2023-05-10     |2023-05-12   |\n",
      "|v1 |c1  |null  |null  |c2    |r3         |2023-05-13 15:00:00|a363a9dd6d5b30865ab5813581941516|2023-05-13     |9999-12-31   |\n",
      "+---+----+------+------+------+-----------+-------------------+--------------------------------+---------------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def df_to_scd2(df, pk_cols, non_pk_cols, time_col):\n",
    "    \"\"\"Функция создания SCD2 историчности таблицы\"\"\"\n",
    "    \n",
    "    tech_col_names = { # для быстрой замены\n",
    "        'row_hash': 'row_hash',\n",
    "        'row_actual_from': 'row_actual_from',\n",
    "        'row_actual_to': 'row_actual_to',\n",
    "    }\n",
    "    \n",
    "    def concat_cols(*cols):\n",
    "        return F.concat_ws('', *sorted(cols)) # сортировка для постоянного хэша\n",
    "    \n",
    "    df_cols = df.columns\n",
    "    \n",
    "    window_pk_asc = W.partitionBy(*pk_cols).orderBy(time_col)\n",
    "    df_hash = (\n",
    "        df\n",
    "        .withColumn('row_hash', F.md5(concat_cols(*pk_cols, *non_pk_cols))) # хэш только по ключу и бизнес-атрибутам\n",
    "        .withColumn('row_actual_from', col(time_col).cast('date'))\n",
    "        .withColumn('version_num', F.count(F.when(F.lag('row_hash').over(window_pk_asc) != col('row_hash'), 1)).over(window_pk_asc))\n",
    "    )\n",
    "    df_ded_by_hash = deduplicate_df(\n",
    "        df_hash,\n",
    "        pk=[*pk_cols, 'version_num'],\n",
    "        order_by_cols=[time_col], # нужна первая запись с хэшом с учетом последовательности изменений\n",
    "    )\n",
    "    df_ded_by_date = deduplicate_df(\n",
    "        df_ded_by_hash,\n",
    "        pk=[*pk_cols, 'row_actual_from'],\n",
    "        order_by_cols=[F.desc(time_col)], # нужна последняя запись в разрезе дня\n",
    "    )\n",
    "    \n",
    "    window_row_actual_to = W.partitionBy(*pk_cols).orderBy('row_actual_from')\n",
    "    \n",
    "    alias_tech_col_names = lambda x: col(x).alias(tech_col_names[x])\n",
    "    \n",
    "    df_result = (\n",
    "        df_ded_by_date\n",
    "        .withColumn(\n",
    "            'row_actual_to', \n",
    "            F.coalesce(F.date_sub(F.lead('row_actual_from').over(window_row_actual_to), 1), F.lit('9999-12-31'))\n",
    "        )\n",
    "        .select(\n",
    "            *df_cols, \n",
    "            alias_tech_col_names('row_hash'), \n",
    "            alias_tech_col_names('row_actual_from').cast('string'), \n",
    "            alias_tech_col_names('row_actual_to').cast('string'),\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    return df_result\n",
    "\n",
    "df_to_scd2(s1, ['pk1', 'pk2'], ['nonpk1', 'nonpk2', 'nonpk3'], 'ts').orderBy(['pk1', 'pk2', 'ts']).show(100, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "35dde350-b242-495f-866a-fcc20fb47a81",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1, df2 = [spark.read.csv(f'scd2_data/df_scd2_join_{i}.csv', sep=';', header=True) for i in range(1, 3)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e43ae19a-c9fc-4025-8abb-626b2d5da4a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+--------+-------------------+\n",
      "|pk1|pk2|phone_id|ts                 |\n",
      "+---+---+--------+-------------------+\n",
      "|v1 |c1 |e1      |2023-04-01 10:00:00|\n",
      "|v1 |c1 |e2      |2023-05-06 12:00:00|\n",
      "|v1 |c1 |e3      |2023-05-12 12:00:00|\n",
      "|v1 |c1 |e1      |2023-05-13 12:00:00|\n",
      "|v1 |c2 |e1      |2023-04-01 10:00:00|\n",
      "|v1 |c2 |e2      |2023-05-06 12:00:00|\n",
      "|v1 |c2 |e3      |2023-05-12 12:00:00|\n",
      "|v1 |c2 |e1      |2023-05-13 12:00:00|\n",
      "+---+---+--------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2.show(10, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d2f67671-94d3-4575-8176-2d8e37dcc021",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1_scd2, df2_scd2 = [df_to_scd2(df, ['pk1', 'pk2'], [non_pk_col], 'ts').orderBy(['pk1', 'pk2', 'ts']).drop('ts') for df, non_pk_col in zip((df1, df2), ('email_id', 'phone_id'))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8b0ac168-a2d1-40ef-a79f-abc0e783f815",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+--------+--------------------------------+---------------+-------------+\n",
      "|pk1|pk2|phone_id|row_hash                        |row_actual_from|row_actual_to|\n",
      "+---+---+--------+--------------------------------+---------------+-------------+\n",
      "|v1 |c1 |e1      |e14f0e80db49cd1501de87adf05f6022|2023-04-01     |2023-05-05   |\n",
      "|v1 |c1 |e2      |9862c1fb9265b03695dc9a727406c43e|2023-05-06     |2023-05-11   |\n",
      "|v1 |c1 |e3      |543b4e1fe15d3cd37fc7b9454156f4e1|2023-05-12     |2023-05-12   |\n",
      "|v1 |c1 |e1      |e14f0e80db49cd1501de87adf05f6022|2023-05-13     |9999-12-31   |\n",
      "|v1 |c2 |e1      |db078b8d7b629e8c3e11aeaf24952480|2023-04-01     |2023-05-05   |\n",
      "|v1 |c2 |e2      |284ed4afc0045d818e840896714656ca|2023-05-06     |2023-05-11   |\n",
      "|v1 |c2 |e3      |87795052bb06129a6007a0dfaad2efef|2023-05-12     |2023-05-12   |\n",
      "|v1 |c2 |e1      |db078b8d7b629e8c3e11aeaf24952480|2023-05-13     |9999-12-31   |\n",
      "+---+---+--------+--------------------------------+---------------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2_scd2.show(100, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3742d05d-0fcd-4dab-9f81-444d50a64840",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_df_scd2(df, pk, ): pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b3cf5f68-f6d8-4172-84fb-1db2b7395c4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+--------+--------+---------------+-------------+\n",
      "|pk1|pk2|email_id|phone_id|row_actual_from|row_actual_to|\n",
      "+---+---+--------+--------+---------------+-------------+\n",
      "|v1 |c1 |e1      |e1      |2023-05-01     |2023-05-03   |\n",
      "|v1 |c1 |e2      |e1      |2023-05-04     |2023-05-05   |\n",
      "|v1 |c1 |e2      |e2      |2023-05-06     |2023-05-09   |\n",
      "|v1 |c1 |e3      |e2      |2023-05-10     |2023-05-11   |\n",
      "|v1 |c1 |e1      |e3      |2023-05-12     |2023-05-12   |\n",
      "|v1 |c1 |e1      |e1      |2023-05-13     |9999-12-31   |\n",
      "|v1 |c2 |e1      |e1      |2023-05-01     |2023-05-05   |\n",
      "|v1 |c2 |e1      |e2      |2023-05-06     |2023-05-11   |\n",
      "|v1 |c2 |e1      |e3      |2023-05-12     |2023-05-12   |\n",
      "|v1 |c2 |e1      |e1      |2023-05-13     |9999-12-31   |\n",
      "+---+---+--------+--------+---------------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def scd2_join(df1, df2, pk):\n",
    "    df1, df2 = df1.alias('df1'), df2.alias('df2')\n",
    "    \n",
    "    tech_attr = {'row_actual_from', 'row_actual_to', 'row_hash'}\n",
    "    def get_non_pk_attrs(df):\n",
    "        all_attrs = set(df.columns)\n",
    "        pk_attrs = set(pk)\n",
    "        non_pk_attrs = all_attrs - tech_attr - pk_attrs\n",
    "        return non_pk_attrs\n",
    "    \n",
    "    greatest_from = F.greatest(df1['row_actual_from'], df2['row_actual_from'])\n",
    "    least_to = F.least(df1['row_actual_to'], df2['row_actual_to'])\n",
    "    pk_cond_join = ' and '.join([f'df1.{pk_col} = df2.{pk_col}' for pk_col in pk])\n",
    "    \n",
    "    cond_scd2_join = F.expr(pk_cond_join) & (greatest_from <= least_to)\n",
    "    df_joined = (\n",
    "        df1\n",
    "        .join(df2, on=cond_scd2_join, how='inner')\n",
    "    )\n",
    "    \n",
    "    df_new_scd2 = (\n",
    "        df_joined\n",
    "        .select(\n",
    "            *[f'df1.{pk_col}' for pk_col in pk], # из-за условия джоина нельзя просто так взять атрибуты из pk\n",
    "            *get_non_pk_attrs(df1),\n",
    "            *get_non_pk_attrs(df2),\n",
    "            greatest_from.alias('row_actual_from'),\n",
    "            least_to.alias('row_actual_to'),\n",
    "        )\n",
    "    )\n",
    "\n",
    "    return df_new_scd2\n",
    "scd2_join(df1_scd2, df2_scd2, ['pk1','pk2']).orderBy('pk1', 'pk2','row_actual_from').show(100, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "50d6db1a-5890-4dcf-a92c-83ea128d88a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
