{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "54b7f6f1-a926-4ae8-824e-dd31e6beeb50",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/12/15 21:06:39 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "22/12/15 21:06:40 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
     ]
    }
   ],
   "source": [
    "from hhop import DFExtender, write_table_through_view, spark, col, F"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c326c58-b373-46a1-9a28-a48adc8b298c",
   "metadata": {},
   "source": [
    "Возможно, пригодятся \n",
    "1. .config(\"spark.hadoop.hive.exec.dynamic.partition\", \"true\") \\\n",
    "2. .config(\"spark.hadoop.hive.exec.dynamic.partition.mode\", \"nonstrict\") \\"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19582cd6-8bba-4576-bafc-4f751384e75b",
   "metadata": {},
   "source": [
    "## Synth table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8ac7ee8a-a703-44f5-86d6-db9a77346caf",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_src = spark.read.csv('./synth_data/table1.csv', header=True, sep=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "aaaa138e-7ef5-4008-a5ee-364cf507e99a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----+----+------+--------+----------+----------+\n",
      "|index| pk1| pk2|  var1|    var2|   dt_part|group_part|\n",
      "+-----+----+----+------+--------+----------+----------+\n",
      "|    1|key1|   1|value1|value2_1|2022-12-15|    group1|\n",
      "|    2|key1|   1|value1|value2_1|2022-12-16|    group2|\n",
      "|    3|key1|   2|value1|value2_1|2022-12-16|    group3|\n",
      "|    4|key2|   2|  null|value2_1|2022-12-17|    group1|\n",
      "|    5|key2|   3|value1|value2_1|2022-12-18|    group2|\n",
      "|    6|key2|   4|value1|    null|2022-12-19|    group3|\n",
      "|    7|key2|null|value1|value2_1|2022-12-19|    group4|\n",
      "|    8|null|   4|value1|value2_1|2022-12-20|    group3|\n",
      "|    9|null|null|value1|value2_1|2022-12-20|    group7|\n",
      "+-----+----+----+------+--------+----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_src.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9bf11d6f-563e-4652-add7-5c15e48ec599",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_src_write = df_src.write.mode('overwrite')\n",
    "\n",
    "df_src_write.partitionBy('dt_part', 'group_part').saveAsTable('default.part_table_test1')\n",
    "df_src_write.saveAsTable('default.nonpart_table_test1')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab0a2abe-01e2-413a-8f04-deaacf93b1fb",
   "metadata": {},
   "source": [
    "## Info about table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e31e4e76-2d82-4da2-af58-0f4dce5dd4bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "table info default.part_table_test1\n",
      "----------------------------------------\n",
      "root\n",
      " |-- index: string (nullable = true)\n",
      " |-- pk1: string (nullable = true)\n",
      " |-- pk2: string (nullable = true)\n",
      " |-- var1: string (nullable = true)\n",
      " |-- var2: string (nullable = true)\n",
      " |-- dt_part: string (nullable = true)\n",
      " |-- group_part: string (nullable = true)\n",
      "\n",
      "cols: ['index', 'pk1', 'pk2', 'var1', 'var2', 'dt_part', 'group_part']\n",
      "partition columns: ['dt_part', 'group_part']\n",
      "see attribute like this for full info: DataFrame.describe_table.show(100, False)\n",
      "\n",
      "location: file:/Users/pyro/github/HiveHelper_on_PySpark/spark-warehouse/part_table_test1\n"
     ]
    }
   ],
   "source": [
    "df = DFExtender('default','part_table_test1', pk=['pk1', 'pk2'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fbf12450-da0b-4ead-95fd-360952ae91ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---------+-------+\n",
      "|col_name|data_type|comment|\n",
      "+--------+---------+-------+\n",
      "|   index|   string|   null|\n",
      "|     pk1|   string|   null|\n",
      "|     pk2|   string|   null|\n",
      "+--------+---------+-------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.describe_table.show(3, 30)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24551538-34b8-422c-a517-2fcf98a7784e",
   "metadata": {},
   "source": [
    "## SchemaManager"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc9db512-0d4c-4b3f-96b8-2e3c5fd3867c",
   "metadata": {},
   "source": [
    "## Comparing tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96b27f14-79ca-407f-b0e8-e3278d420cc6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
