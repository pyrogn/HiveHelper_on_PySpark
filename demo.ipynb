{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "54b7f6f1-a926-4ae8-824e-dd31e6beeb50",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/12/18 18:51:48 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '/Users/pyro/github/HiveHelper_on_PySpark/hhop') \n",
    "# for running .ipynb files anywhere outside of a current dir using the module hhop\n",
    "\n",
    "import hhop # custom module\n",
    "from hhop import DFExtender, SchemaManager #main classes\n",
    "from funs import read_table, write_table_through_view, union_all # useful functions\n",
    "\n",
    "from functools import reduce\n",
    "from importlib import reload\n",
    "import pandas as pd\n",
    "\n",
    "from pyspark.sql.functions import col\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.window import Window as W\n",
    "\n",
    "import shutil, os, time # working with FS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c326c58-b373-46a1-9a28-a48adc8b298c",
   "metadata": {},
   "source": [
    "Возможно, пригодятся \n",
    "1. .config(\"spark.hadoop.hive.exec.dynamic.partition\", \"true\") \\\n",
    "2. .config(\"spark.hadoop.hive.exec.dynamic.partition.mode\", \"nonstrict\") \\"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19582cd6-8bba-4576-bafc-4f751384e75b",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Synth table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8ac7ee8a-a703-44f5-86d6-db9a77346caf",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_src = spark.read.csv('./synth_data/table1.csv', header=True, sep=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aaaa138e-7ef5-4008-a5ee-364cf507e99a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----+----+------+--------+----------+----------+\n",
      "|index| pk1| pk2|  var1|    var2|   dt_part|group_part|\n",
      "+-----+----+----+------+--------+----------+----------+\n",
      "|    1|key1|   1|value1|value2_1|2022-12-15|    group1|\n",
      "|    2|key1|   1|value1|value2_1|2022-12-16|    group2|\n",
      "|    3|key1|   2|value1|value2_1|2022-12-16|    group3|\n",
      "|    4|key2|   2|  null|value2_1|2022-12-17|    group1|\n",
      "|    5|key2|   3|value1|value2_1|2022-12-18|    group2|\n",
      "|    6|key2|   4|value1|    null|2022-12-19|    group3|\n",
      "|    7|key2|null|value1|value2_1|2022-12-19|    group4|\n",
      "|    8|null|   4|value1|value2_1|2022-12-20|    group3|\n",
      "|    9|null|null|value1|value2_1|2022-12-20|    group7|\n",
      "+-----+----+----+------+--------+----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_src.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9bf11d6f-563e-4652-add7-5c15e48ec599",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df_src.write.mode('overwrite').partitionBy('dt_part', 'group_part').saveAsTable('default.part_table_test1')\n",
    "df_src.write.mode('overwrite').saveAsTable('default.nonpart_table_test1')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab0a2abe-01e2-413a-8f04-deaacf93b1fb",
   "metadata": {},
   "source": [
    "## Info about table"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d11616d9-de5d-4dde-8f96-ceab126beb13",
   "metadata": {},
   "source": [
    "### Reading table from Hive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "77c4f3ce-0d4b-4174-925f-1b6f5025ca02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- index: string (nullable = true)\n",
      " |-- pk1: string (nullable = true)\n",
      " |-- pk2: string (nullable = true)\n",
      " |-- var1: string (nullable = true)\n",
      " |-- var2: string (nullable = true)\n",
      " |-- dt_part: string (nullable = true)\n",
      " |-- group_part: string (nullable = true)\n",
      "\n",
      "location: file:/Users/pyro/github/HiveHelper_on_PySpark/spark-warehouse/part_table_test1\n",
      "partition columns: ['dt_part', 'group_part']\n"
     ]
    }
   ],
   "source": [
    "df_hive = read_table('default.part_table_test1', verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e31e4e76-2d82-4da2-af58-0f4dce5dd4bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = DFExtender(df_hive, pk=['pk1', 'pk2'], verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30a374fc-f207-4e69-9142-9a432aec6d8e",
   "metadata": {},
   "source": [
    "### NULL checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "644274f0-c198-49d3-8474-37f49bff72e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_filled = df_hive.fillna({'pk1': 'default', 'pk2': 'default'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2d654862-3ae7-4c1f-9dd6-6025aa697be8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-------+-------+------+--------+----------+----------+\n",
      "|index|    pk1|    pk2|  var1|    var2|   dt_part|group_part|\n",
      "+-----+-------+-------+------+--------+----------+----------+\n",
      "|    1|   key1|      1|value1|value2_1|2022-12-15|    group1|\n",
      "|    2|   key1|      1|value1|value2_1|2022-12-16|    group2|\n",
      "|    3|   key1|      2|value1|value2_1|2022-12-16|    group3|\n",
      "|    5|   key2|      3|value1|value2_1|2022-12-18|    group2|\n",
      "|    7|   key2|default|value1|value2_1|2022-12-19|    group4|\n",
      "|    8|default|      4|value1|value2_1|2022-12-20|    group3|\n",
      "|    4|   key2|      2|  null|value2_1|2022-12-17|    group1|\n",
      "|    9|default|default|value1|value2_1|2022-12-20|    group7|\n",
      "|    6|   key2|      4|value1|    null|2022-12-19|    group3|\n",
      "+-----+-------+-------+------+--------+----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_filled.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "764b69ff-436b-497a-92fb-21ace914e09c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You can access DF with PK duplicates in attribute `.df_duplicates_pk`\n",
      "\n",
      "Count all:                9\n",
      "Unique PK count:          8\n",
      "PK with duplicates:       1\n",
      "\n",
      "Null values in columns - {'column': [count NULL, share NULL]}:\n",
      "{'var1': [1, 0.1111], 'var2': [1, 0.1111]}\n"
     ]
    }
   ],
   "source": [
    "# reload(hhop)\n",
    "# from hhop import DFExtender\n",
    "k = DFExtender(df_filled, pk=['pk1', 'pk2'], verbose=True)\n",
    "k.getInfo()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2f097bbe-6a66-425b-9582-d191785fd189",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_out = k.getDFWithNull(['var1', 'var2'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0ea5bd21-e2e3-4418-acbf-f5c3415b2eac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----+---+------+--------+----------+----------+---------+\n",
      "|index| pk1|pk2|  var1|    var2|   dt_part|group_part|cnt_nulls|\n",
      "+-----+----+---+------+--------+----------+----------+---------+\n",
      "|    4|key2|  2|  null|value2_1|2022-12-17|    group1|        1|\n",
      "|    6|key2|  4|value1|    null|2022-12-19|    group3|        1|\n",
      "+-----+----+---+------+--------+----------+----------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_out.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea14c1d7-303f-4b7f-8ad9-96b7bf78c06f",
   "metadata": {},
   "source": [
    "### PK checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "820e8d38-2962-4e40-9424-55388a8a8753",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You can access DF with PK duplicates in attribute `.df_duplicates_pk`\n",
      "\n",
      "Count all:                9\n",
      "Unique PK count:          8\n",
      "PK with duplicates:       1\n",
      "PK column 'pk1' contains empty values, be careful!\n",
      "PK column 'pk2' contains empty values, be careful!\n",
      "\n",
      "Null values in columns - {'column': [count NULL, share NULL]}:\n",
      "{'pk1': [2, 0.2222], 'pk2': [2, 0.2222], 'var1': [1, 0.1111], 'var2': [1, 0.1111]}\n"
     ]
    }
   ],
   "source": [
    "df_check = DFExtender(df_hive, ['pk1', 'pk2'], verbose=True)\n",
    "df_check.getInfo()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "73fd79c0-1922-4faf-bd48-c5967f364ec4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----+---+------+--------+----------+----------+------+\n",
      "|index| pk1|pk2|  var1|    var2|   dt_part|group_part|cnt_pk|\n",
      "+-----+----+---+------+--------+----------+----------+------+\n",
      "|    1|key1|  1|value1|value2_1|2022-12-15|    group1|     2|\n",
      "|    2|key1|  1|value1|value2_1|2022-12-16|    group2|     2|\n",
      "+-----+----+---+------+--------+----------+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_check.df_duplicates_pk.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d5bcde33-f8ec-40b1-949f-93e6c921d9de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+-----+\n",
      "| pk1| pk2|count|\n",
      "+----+----+-----+\n",
      "|key1|   1|    2|\n",
      "|null|null|    1|\n",
      "|key2|null|    1|\n",
      "|key1|   2|    1|\n",
      "|key2|   4|    1|\n",
      "|key2|   2|    1|\n",
      "|null|   4|    1|\n",
      "|key2|   3|    1|\n",
      "+----+----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# should i calculate PK duplicates also like this?\n",
    "df.groupBy(['pk1', 'pk2']).count().orderBy(col('count').desc()).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93ccdd9c-255d-4c80-911a-9c544c826eb6",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Comparing tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "96b27f14-79ca-407f-b0e8-e3278d420cc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_synth_sample(name):\n",
    "    (\n",
    "        spark.read.csv(f'./synth_data/{name}.csv', header=True, sep=';')\n",
    "        .write.mode('overwrite')\n",
    "        .partitionBy('dt_part', 'group_part')\n",
    "        .saveAsTable(f'default.{name}')\n",
    "    )\n",
    "    \n",
    "write_synth_sample('table1_comp')  \n",
    "write_synth_sample('table2_comp')\n",
    "\n",
    "df = read_table('default.table1_comp', alias='main')\n",
    "df_ref = read_table('default.table2_comp', alias='ref')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c20e2c3b-32f8-4880-8c6f-36b27d619a12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For testing DFs without common columns outside of PK\n",
    "# df=df.select(['pk1', 'pk2'])\n",
    "# df_ref=df_ref.select(['pk1', 'pk2'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "dc2060b0-65b1-4bd9-9885-034e870b5eeb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Main DF\n",
      "Count all:                6\n",
      "Unique PK count:          6\n",
      "PK with duplicates:       0\n",
      "\n",
      "Reference DF\n",
      "Count all:                6\n",
      "Unique PK count:          6\n",
      "PK with duplicates:       0\n",
      "\n",
      "Errors in columns - {'column': [count is_error, share is_error]}\n",
      "{'group_part': [2, 0.4], 'var1': [2, 0.4], 'var2': [1, 0.2]}\n",
      "\n",
      "Count stats of matching main and reference tables:\n",
      "not in main table:        1\n",
      "not in reference table:   1\n",
      "correct matching:         5\n"
     ]
    }
   ],
   "source": [
    "# reload(hhop)\n",
    "# from hhop import DFExtender\n",
    "\n",
    "df_main = DFExtender(df, pk=['pk1', 'pk2'], verbose=True)\n",
    "df_main.compareTables(df_ref)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bd68b215-1932-4a6e-a7e9-8922fa9760a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---+---------------+------------+----------+---------+---------+--------------+-----------+---------+--------------+---------+------------------+---------------+-------------+------------+------------+--------------+-------------+----------+\n",
      "| pk1|pk2|group_part_main|dt_part_main|index_main|var1_main|var2_main|group_part_ref|dt_part_ref|index_ref|      var1_ref| var2_ref|group_part_is_diff|dt_part_is_diff|index_is_diff|var1_is_diff|var2_is_diff|is_joined_main|is_joined_ref|sum_errors|\n",
      "+----+---+---------------+------------+----------+---------+---------+--------------+-----------+---------+--------------+---------+------------------+---------------+-------------+------------+------------+--------------+-------------+----------+\n",
      "|key1|  1|         group2|  2022-12-15|         1|   value1| value2_1|        group7| 2022-12-15|        1|       value19| value2_1|                 1|              0|            0|           1|           0|             1|            1|         2|\n",
      "|key2|  2|         group2|  2022-12-18|         5|   value1| value2_1|        group2| 2022-12-18|        5|        value1| value2_1|                 0|              0|            0|           0|           0|             1|            1|         0|\n",
      "|key1|  3|         group3|  2022-12-16|         3|   value1| value2_1|        group3| 2022-12-16|        3|        value1| value2_1|                 0|              0|            0|           0|           0|             1|            1|         0|\n",
      "|key2|  3|         group3|  2022-12-20|         6|   value1|     null|          null|       null|     null|          null|     null|                 0|              0|            0|           0|           0|             1|         null|         0|\n",
      "|key2|  1|         group1|  2022-12-17|         4|     null| value2_1|       group10| 2022-12-17|        4|value_not_null| value2_1|                 1|              0|            0|           1|           0|             1|            1|         2|\n",
      "|key2|  5|           null|        null|      null|     null|     null|        group3| 2022-12-20|        6|        value1|     null|                 0|              0|            0|           0|           0|          null|            1|         0|\n",
      "|key1|  2|         group2|  2022-12-16|         2|   value1| value2_1|        group2| 2022-12-16|        2|        value1|value2_55|                 0|              0|            0|           0|           1|             1|            1|         1|\n",
      "+----+---+---------------+------------+----------+---------+---------+--------------+-----------+---------+--------------+---------+------------------+---------------+-------------+------------+------------+--------------+-------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_main.df_with_errors.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24551538-34b8-422c-a517-2fcf98a7784e",
   "metadata": {},
   "source": [
    "## SchemaManager"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac0dcafa-9751-414f-a08d-43b9cce4628a",
   "metadata": {},
   "source": [
    "### dropping empty tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e680e417-3e61-4615-a855-9b91fcc95e72",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "schema_name = 'popular_schema'\n",
    "spark.sql(f\"drop database if exists {schema_name} cascade\")\n",
    "spark.sql(f'create database {schema_name}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d8544bc1-11f5-4734-ae7d-fef940760aeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_src_write = df_src.write.mode('overwrite')\n",
    "df_src_write.partitionBy('dt_part', 'group_part').saveAsTable(f'{schema_name}.table1')\n",
    "df_src_write.saveAsTable(f'{schema_name}.table2')\n",
    "df_src_write.saveAsTable(f'{schema_name}.table3')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "046ce783-1957-4964-843b-243aeff49fe0",
   "metadata": {},
   "source": [
    "table1 has dir + data   \n",
    "table2 has only root dir    \n",
    "table3 doesn't have any dir and data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "dd7a039e-d00b-4a15-ac0b-d6dd80b3edee",
   "metadata": {},
   "outputs": [],
   "source": [
    "shutil.rmtree('./spark-warehouse/popular_schema.db/table2', ignore_errors=True)\n",
    "os.makedirs('./spark-warehouse/popular_schema.db/table2')\n",
    "shutil.rmtree('./spark-warehouse/popular_schema.db/table3', ignore_errors=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1bb55ebf-3739-48d7-b587-0d4d494d9746",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+---------+-----------+\n",
      "|database      |tableName|isTemporary|\n",
      "+--------------+---------+-----------+\n",
      "|popular_schema|table1   |false      |\n",
      "|popular_schema|table2   |false      |\n",
      "|popular_schema|table3   |false      |\n",
      "+--------------+---------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(f\"show tables in {schema_name}\").show(10, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f293f40f-a96c-465d-b66a-ac90a3cc6b52",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 116:>                                                        (0 + 3) / 3]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 tables in popular_schema\n",
      "run drop_empty_tables() on instance to drop empty tables in popular_schema\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "popular_schema = SchemaManager('popular_schema')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "63446417-41ab-44e0-8b99-4d3042ffa3d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 going to be dropped out of 3 (66.67%). Schema: popular_schema\n",
      "After dropping tables there are 1 tables in popular_schema\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/12/18 18:52:09 ERROR FileUtils: Failed to delete file:/Users/pyro/github/HiveHelper_on_PySpark/spark-warehouse/popular_schema.db/table3\n"
     ]
    }
   ],
   "source": [
    "popular_schema.drop_empty_tables()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7f657d0b-94b2-4d22-97e8-8f028e74f8ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+---------+-----------+\n",
      "|database      |tableName|isTemporary|\n",
      "+--------------+---------+-----------+\n",
      "|popular_schema|table1   |false      |\n",
      "+--------------+---------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(f\"show tables in {schema_name}\").show(10, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "684b3ecb-9834-47bc-94c8-f40fa96547a2",
   "metadata": {},
   "source": [
    "## Extra"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27775b51-7ee1-46b0-926e-ea7865d2332e",
   "metadata": {},
   "source": [
    "### function `union_all`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "689ee851-10e8-42be-ab36-6c7991c952e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generating list of DataFrames\n",
    "list_dfs = []\n",
    "values = [\n",
    "        (\"x\",\"x\"),\n",
    "        (\"x\",\"y\"),\n",
    "        (\"x\",None),\n",
    "        (None,\"x\"),\n",
    "        (None,None),\n",
    "    ]\n",
    "columns = ['val1', 'val2']\n",
    "for val1, val2 in ((1,1), (1, None), (None, 1)):\n",
    "    df_test = spark.createDataFrame(values, columns)\n",
    "    df_test = (\n",
    "        df_test\n",
    "        .withColumn('is_joined_main', F.lit(val1))\n",
    "        .withColumn('is_joined_ref', F.lit(val2))\n",
    "    )\n",
    "    list_dfs.append(df_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "076d0c97-4553-4da8-8236-76f101aff7df",
   "metadata": {},
   "source": [
    "You can pass any number of Spark DataFrames is `union_all()`  \n",
    "Requirements:\n",
    "1. all DFs must have same columns\n",
    "2. If `dfs` is a list, explode it like `*dfs` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "789389e1-89e0-4921-8d02-33e04909e9ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[DataFrame[val1: string, val2: string, is_joined_main: int, is_joined_ref: int],\n",
       " DataFrame[val1: string, val2: string, is_joined_main: int, is_joined_ref: null],\n",
       " DataFrame[val1: string, val2: string, is_joined_main: null, is_joined_ref: int]]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(list_dfs))\n",
    "list_dfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b491b0f1-8e71-490b-aeca-457f74314fac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[val1: string, val2: string, is_joined_main: int, is_joined_ref: int]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_from_union = union_all(*list_dfs).cache()\n",
    "# union_all(list_dfs[0], list_dfs[1], list_dfs[2]) # equivalent\n",
    "df_from_union"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "075be460-e253-4f11-9bdd-87a91274b9ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+--------------+-------------+-------+\n",
      "|val1|val2|is_joined_main|is_joined_ref|is_diff|\n",
      "+----+----+--------------+-------------+-------+\n",
      "|   x|   x|             1|            1|      0|\n",
      "|   x|   y|             1|            1|      1|\n",
      "|   x|null|             1|            1|      1|\n",
      "|null|   x|             1|            1|      1|\n",
      "|null|null|             1|            1|      0|\n",
      "|   x|   x|             1|         null|      0|\n",
      "|   x|   y|             1|         null|      0|\n",
      "|   x|null|             1|         null|      0|\n",
      "|null|   x|             1|         null|      0|\n",
      "|null|null|             1|         null|      0|\n",
      "|   x|   x|          null|            1|      0|\n",
      "|   x|   y|          null|            1|      0|\n",
      "|   x|null|          null|            1|      0|\n",
      "|null|   x|          null|            1|      0|\n",
      "|null|null|          null|            1|      0|\n",
      "+----+----+--------------+-------------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# this is exactly a filter in the script for comparing tables\n",
    "dummy1, dummy2,val1,val2='is_joined_main','is_joined_ref','val1','val2'\n",
    "cond_diff = f\"\"\"case\n",
    "            when\n",
    "                ({dummy1} is null or {dummy2} is null) \n",
    "                or\n",
    "                ({val1} is null and {val2} is null)\n",
    "                or \n",
    "                ({val1} = {val2})\n",
    "                then 0\n",
    "                else 1\n",
    "            end\"\"\"\n",
    "(\n",
    "    df_from_union\n",
    "    .withColumn('is_diff', F.expr(cond_diff))\n",
    "    .show(100)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0c8d6e6-249a-4948-8bf4-9c597fb92afa",
   "metadata": {},
   "source": [
    "splitting big dataset if applicable"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc6084b6-4744-49ae-b885-046e7bf424d0",
   "metadata": {},
   "source": [
    "writing tables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdd0263d-7f7e-453f-b164-7b6bf9a723b6",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Modification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a97c0be-a9f4-417e-bb02-5598fbbdaf43",
   "metadata": {},
   "source": [
    "1. read as you like, use DFExtender to get stats\n",
    "2. use all methods from PySpark as usual (but they convert DF from DFExtender to DataFrame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "bffc22ab-b020-4305-b6a7-3c917f920c52",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = read_table('default.table1_comp', alias='main')\n",
    "df_main = DFExtender(df, pk=['pk1', 'pk2'], verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "500d2dd6-b71c-48ff-ada8-a8aa264301a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'hhop.DFExtender'>\n"
     ]
    }
   ],
   "source": [
    "print(df_main.__class__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "99d35d00-8d16-467e-9737-f6d0efa8528a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_main_filter = df_main.filter(col('pk1').isNotNull())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "4dafcedb-abb6-43a4-95b4-8438ebd58cdb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pyspark.sql.dataframe.DataFrame'>\n"
     ]
    }
   ],
   "source": [
    "print(df_main_filter.__class__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90bd05ac-3f55-43d3-83a9-997710829417",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
