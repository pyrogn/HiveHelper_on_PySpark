{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "54b7f6f1-a926-4ae8-824e-dd31e6beeb50",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/12/21 00:17:44 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '/Users/pyro/github/HiveHelper_on_PySpark/hhop') \n",
    "# for running .ipynb files anywhere outside of a current dir using the module hhop\n",
    "\n",
    "from functools import reduce\n",
    "from importlib import reload\n",
    "import pandas as pd\n",
    "\n",
    "from pyspark.sql.functions import col\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.window import Window as W\n",
    "\n",
    "import shutil, os, time # working with FS\n",
    "\n",
    "import hhop # custom module\n",
    "from hhop import DFExtender, SchemaManager #main classes\n",
    "import funs\n",
    "from funs import read_table, write_table, union_all # useful functions\n",
    "from spark_init import spark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19582cd6-8bba-4576-bafc-4f751384e75b",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Creating a synth table from csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8ac7ee8a-a703-44f5-86d6-db9a77346caf",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_src = spark.read.csv('./synth_data/table1.csv', header=True, sep=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aaaa138e-7ef5-4008-a5ee-364cf507e99a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----+----+------+--------+----------+----------+\n",
      "|index| pk1| pk2|  var1|    var2|   dt_part|group_part|\n",
      "+-----+----+----+------+--------+----------+----------+\n",
      "|    1|key1|   1|value1|value2_1|2022-12-15|    group1|\n",
      "|    2|key1|   1|value1|value2_1|2022-12-16|    group2|\n",
      "|    3|key1|   2|value1|value2_1|2022-12-16|    group3|\n",
      "|    4|key2|   2|  null|value2_1|2022-12-17|    group1|\n",
      "|    5|key2|   3|value1|value2_1|2022-12-18|    group2|\n",
      "|    6|key2|   4|value1|    null|2022-12-19|    group3|\n",
      "|    7|key2|null|value1|value2_1|2022-12-19|    group4|\n",
      "|    8|null|   4|value1|value2_1|2022-12-20|    group3|\n",
      "|    9|null|null|value1|value2_1|2022-12-20|    group7|\n",
      "+-----+----+----+------+--------+----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_src.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9bf11d6f-563e-4652-add7-5c15e48ec599",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df_src.write.mode('overwrite').partitionBy('dt_part', 'group_part').saveAsTable('default.part_table_test1')\n",
    "df_src.repartition(4).write.mode('overwrite').saveAsTable('default.nonpart_table_test1')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab0a2abe-01e2-413a-8f04-deaacf93b1fb",
   "metadata": {},
   "source": [
    "## Info about table"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d11616d9-de5d-4dde-8f96-ceab126beb13",
   "metadata": {},
   "source": [
    "### Reading table from Hive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "77c4f3ce-0d4b-4174-925f-1b6f5025ca02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- index: string (nullable = true)\n",
      " |-- pk1: string (nullable = true)\n",
      " |-- pk2: string (nullable = true)\n",
      " |-- var1: string (nullable = true)\n",
      " |-- var2: string (nullable = true)\n",
      " |-- dt_part: string (nullable = true)\n",
      " |-- group_part: string (nullable = true)\n",
      "\n",
      "partition columns: ['dt_part', 'group_part']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "location: file:/Users/pyro/github/HiveHelper_on_PySpark/spark-warehouse/part_table_test1\n",
      "9 parquet files at the location\n"
     ]
    }
   ],
   "source": [
    "df = read_table('default.part_table_test1', verbose=True, cnt_files=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2da7742e-b7a7-4e13-9827-8e2b106a55f8",
   "metadata": {},
   "source": [
    "**Whenever you get a DF from DFExtender do not forget to either**  \n",
    "1. write to Hive using custom function `write_table(df, table, ...)`\n",
    "2. write to Hive using native Spark methods: `df.write.mode('overwrite').saveAsTable('schema.table')`\n",
    "3. or cache DF like `df = df.cache()`\n",
    "\n",
    "\n",
    "Otherwise Spark will read sources of this DF every time and it could be very time consuming."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bdd6edd-00c2-4575-bebc-2ee432eb8234",
   "metadata": {
    "tags": []
   },
   "source": [
    "### NULL checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "764b69ff-436b-497a-92fb-21ace914e09c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You can access DF with PK duplicates in an attribute `.df_duplicates_pk`\n",
      "\n",
      "Count all:                9\n",
      "Unique PK count:          8\n",
      "PK with duplicates:       1\n",
      "PK column 'pk1' contains empty values, be careful!\n",
      "PK column 'pk2' contains empty values, be careful!\n",
      "\n",
      "Null values in columns - {'column': [count NULL, share NULL]}:\n",
      "{'pk1': [2, 0.2222], 'pk2': [2, 0.2222], 'var1': [1, 0.1111], 'var2': [1, 0.1111]}\n"
     ]
    }
   ],
   "source": [
    "# DFExtender doesn't change DataFrame during initialization and returns it as is\n",
    "df_check = DFExtender(df, pk=['pk1', 'pk2'], verbose=True)\n",
    "df_check.get_info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2f097bbe-6a66-425b-9582-d191785fd189",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this method returns a DF sorted by count of nulls in selected columns in descending order\n",
    "df_check_null = df_check.get_df_with_null(['var1', 'var2'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0ea5bd21-e2e3-4418-acbf-f5c3415b2eac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----+---+------+--------+----------+----------+---------+\n",
      "|index| pk1|pk2|  var1|    var2|   dt_part|group_part|cnt_nulls|\n",
      "+-----+----+---+------+--------+----------+----------+---------+\n",
      "|    6|key2|  4|value1|    null|2022-12-19|    group3|        1|\n",
      "|    4|key2|  2|  null|value2_1|2022-12-17|    group1|        1|\n",
      "+-----+----+---+------+--------+----------+----------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_check_null.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea14c1d7-303f-4b7f-8ad9-96b7bf78c06f",
   "metadata": {},
   "source": [
    "### PK checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "820e8d38-2962-4e40-9424-55388a8a8753",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You can access DF with PK duplicates in an attribute `.df_duplicates_pk`\n",
      "\n",
      "Count all:                9\n",
      "Unique PK count:          8\n",
      "PK with duplicates:       1\n",
      "PK column 'pk1' contains empty values, be careful!\n",
      "PK column 'pk2' contains empty values, be careful!\n",
      "\n",
      "Null values in columns - {'column': [count NULL, share NULL]}:\n",
      "{'pk1': [2, 0.2222], 'pk2': [2, 0.2222], 'var1': [1, 0.1111], 'var2': [1, 0.1111]}\n"
     ]
    }
   ],
   "source": [
    "df_check = DFExtender(df, pk=['pk1', 'pk2'], verbose=True)\n",
    "df_check.get_info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "73fd79c0-1922-4faf-bd48-c5967f364ec4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----+---+------+--------+----------+----------+------+\n",
      "|index| pk1|pk2|  var1|    var2|   dt_part|group_part|cnt_pk|\n",
      "+-----+----+---+------+--------+----------+----------+------+\n",
      "|    1|key1|  1|value1|value2_1|2022-12-15|    group1|     2|\n",
      "|    2|key1|  1|value1|value2_1|2022-12-16|    group2|     2|\n",
      "+-----+----+---+------+--------+----------+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_check.df_duplicates_pk.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d5bcde33-f8ec-40b1-949f-93e6c921d9de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+-----+\n",
      "| pk1| pk2|count|\n",
      "+----+----+-----+\n",
      "|key1|   1|    2|\n",
      "|key2|   4|    1|\n",
      "|key2|   3|    1|\n",
      "|key2|null|    1|\n",
      "|key2|   2|    1|\n",
      "|null|   4|    1|\n",
      "|key1|   2|    1|\n",
      "|null|null|    1|\n",
      "+----+----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.groupBy(['pk1', 'pk2']).count().orderBy(col('count').desc()).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93ccdd9c-255d-4c80-911a-9c544c826eb6",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Comparing tables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6f3b51e-7ec3-40f0-abba-388fe7fbedab",
   "metadata": {},
   "source": [
    "Sometimes you need to compare two tables based on its primary keys.    \n",
    "This method does exactly that. It\n",
    "1. joins two DFs\n",
    "2. calculates statistics from `DFExtender.get_info()`\n",
    "3. print statistics on joining two tables, errors on non-PK attributes\n",
    "4. returns DF with errors for manual analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "96b27f14-79ca-407f-b0e8-e3278d420cc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_synth_sample(name):\n",
    "    (\n",
    "        spark.read.csv(f'./synth_data/{name}.csv', header=True, sep=';')\n",
    "        .write.mode('overwrite')\n",
    "        .partitionBy('dt_part', 'group_part')\n",
    "        .saveAsTable(f'default.{name}')\n",
    "    )\n",
    "    \n",
    "write_synth_sample('table1_comp')  \n",
    "write_synth_sample('table2_comp')\n",
    "\n",
    "df = read_table('default.table1_comp', alias='main')\n",
    "df_ref = read_table('default.table2_comp', alias='ref')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c20e2c3b-32f8-4880-8c6f-36b27d619a12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For testing DFs without common columns outside of PK\n",
    "# df=df.select(['pk1', 'pk2'])\n",
    "# df_ref=df_ref.select(['pk1', 'pk2'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ffc4a6e-9cbd-4927-ba2b-e0e818e4a575",
   "metadata": {},
   "source": [
    "Instance of DFExtender is the **main DF**,   \n",
    "DF in arguments is the **reference DF**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "dc2060b0-65b1-4bd9-9885-034e870b5eeb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Main DF\n",
      "Count all:                6\n",
      "Unique PK count:          6\n",
      "PK with duplicates:       0\n",
      "\n",
      "Reference DF\n",
      "Count all:                6\n",
      "Unique PK count:          6\n",
      "PK with duplicates:       0\n",
      "\n",
      "Errors in columns - {'column': [count is_error, share is_error]}\n",
      "{'var1': [2, 0.4], 'group_part': [2, 0.4], 'var2': [1, 0.2]}\n",
      "\n",
      "Count stats of matching main and reference tables:\n",
      "not in main table:        1\n",
      "not in reference table:   1\n",
      "correct matching:         5\n"
     ]
    }
   ],
   "source": [
    "df_main = DFExtender(df, pk=['pk1', 'pk2'], verbose=True)\n",
    "df_main.compare_tables(df_ref)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bd68b215-1932-4a6e-a7e9-8922fa9760a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_matching_errors = df_main.df_with_errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4f90fdd1-ad48-47e9-a6a9-7b06bc562356",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# filter for rows that are \"not in main table\"\n",
    "df_matching_errors.filter(col('is_joined_main').isNull())\\\n",
    ".count() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7d94714c-8ab5-48c5-ae5b-367d8738824d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# filter for rows that are \"not in reference table\"\n",
    "df_matching_errors.filter(col('is_joined_ref').isNull())\\\n",
    ".count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d940e2ce-b030-4887-90a0-94b549a56d42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+---------+--------------+\n",
      "|var1_is_diff|var1_main|      var1_ref|\n",
      "+------------+---------+--------------+\n",
      "|           1|   value1|       value19|\n",
      "|           1|     null|value_not_null|\n",
      "+------------+---------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# filter for finding an exact difference in column\n",
    "df_matching_errors.filter(col('var1_is_diff') == 1).select('var1_is_diff', 'var1_main', 'var1_ref').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24551538-34b8-422c-a517-2fcf98a7784e",
   "metadata": {},
   "source": [
    "## SchemaManager"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac0dcafa-9751-414f-a08d-43b9cce4628a",
   "metadata": {},
   "source": [
    "This class provides an interface for analyzing how many tables in a schema don't have underlying folders or any data.  \n",
    "Then you can drop empty or broken tables from the selected schema."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e680e417-3e61-4615-a855-9b91fcc95e72",
   "metadata": {},
   "outputs": [],
   "source": [
    "schema_name = 'popular_schema' # our chosen schema for inspection\n",
    "spark.sql(f\"drop database if exists {schema_name} cascade\")\n",
    "spark.sql(f'create database {schema_name}')\n",
    "None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d8544bc1-11f5-4734-ae7d-fef940760aeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_src_write = df_src.write.mode('overwrite')\n",
    "df_src_write.partitionBy('dt_part', 'group_part').saveAsTable(f'{schema_name}.table1')\n",
    "df_src_write.saveAsTable(f'{schema_name}.table2')\n",
    "df_src_write.saveAsTable(f'{schema_name}.table3')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "046ce783-1957-4964-843b-243aeff49fe0",
   "metadata": {},
   "source": [
    "We created 3 sample tables:\n",
    "\n",
    "1. table1 has dir + data. It **won't** be deleted.  \n",
    "2. table2 has only root dir and no data. It **will** be deleted.  \n",
    "3. table3 doesn't have any dir and data. It **will** be deleted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "dd7a039e-d00b-4a15-ac0b-d6dd80b3edee",
   "metadata": {},
   "outputs": [],
   "source": [
    "table2_path = './spark-warehouse/popular_schema.db/table2'\n",
    "table3_path = './spark-warehouse/popular_schema.db/table3'\n",
    "\n",
    "shutil.rmtree(table2_path, ignore_errors=True)\n",
    "os.makedirs(table2_path)\n",
    "\n",
    "shutil.rmtree(table3_path, ignore_errors=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1bb55ebf-3739-48d7-b587-0d4d494d9746",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+---------+-----------+\n",
      "|database      |tableName|isTemporary|\n",
      "+--------------+---------+-----------+\n",
      "|popular_schema|table1   |false      |\n",
      "|popular_schema|table2   |false      |\n",
      "|popular_schema|table3   |false      |\n",
      "+--------------+---------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(f\"show tables in {schema_name}\").show(10, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f293f40f-a96c-465d-b66a-ac90a3cc6b52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 tables in popular_schema\n",
      "run find_empty_tables() on instance to find empty tables in popular_schema\n"
     ]
    }
   ],
   "source": [
    "popular_schema = SchemaManager('popular_schema')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4cb9fad1-b0e4-44ba-b38e-7b1215458837",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 going to be dropped out of 3 (66.67%)\n",
      "Data about tables stored in attribute '.dict_of_tables':\n",
      "1 - has data, 0 - doesn't and going to be deleted\n",
      "\n",
      "run drop_empty_tables() on instance to drop empty tables in popular_schema\n"
     ]
    }
   ],
   "source": [
    "popular_schema.find_empty_tables()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7e1072c1-a9e0-4d65-9ace-07e7af96f5d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'table1': 1, 'table2': 0, 'table3': 0}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "popular_schema.dict_of_tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "63446417-41ab-44e0-8b99-4d3042ffa3d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After dropping tables there are 1 tables in popular_schema\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/12/21 00:18:06 ERROR FileUtils: Failed to delete file:/Users/pyro/github/HiveHelper_on_PySpark/spark-warehouse/popular_schema.db/table3\n"
     ]
    }
   ],
   "source": [
    "popular_schema.drop_empty_tables()\n",
    "# errors are OK, because sometimes you need to remove folders, but data in Metastore stays the same"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7f657d0b-94b2-4d22-97e8-8f028e74f8ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+---------+-----------+\n",
      "|database      |tableName|isTemporary|\n",
      "+--------------+---------+-----------+\n",
      "|popular_schema|table1   |false      |\n",
      "+--------------+---------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(f\"show tables in {schema_name}\").show(10, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "684b3ecb-9834-47bc-94c8-f40fa96547a2",
   "metadata": {},
   "source": [
    "## Extra"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27775b51-7ee1-46b0-926e-ea7865d2332e",
   "metadata": {},
   "source": [
    "### function `union_all`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60c6b869-5fe4-4551-b65a-224862a89878",
   "metadata": {},
   "source": [
    "This function allows making a union operation of any number of Spark DataFrames  \n",
    "Requirements:\n",
    "1. all DFs must have same columns\n",
    "2. If `dfs` is a list, explode it like `*dfs` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "689ee851-10e8-42be-ab36-6c7991c952e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generating list of 3 DataFrames (5 row count each)\n",
    "list_dfs = []\n",
    "values = [\n",
    "        (\"x\",\"x\"),\n",
    "        (\"x\",\"y\"),\n",
    "        (\"x\",None),\n",
    "        (None,\"x\"),\n",
    "        (None,None),\n",
    "    ]\n",
    "columns = ['val1', 'val2']\n",
    "for val1, val2 in ((1,1), (1, None), (None, 1)):\n",
    "    df_test = spark.createDataFrame(values, columns)\n",
    "    df_test = (\n",
    "        df_test\n",
    "        .withColumn('is_joined_main', F.lit(val1))\n",
    "        .withColumn('is_joined_ref', F.lit(val2))\n",
    "    )\n",
    "    list_dfs.append(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b61c4c0b-2b63-4e59-81e9-adef55120e46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count of 1 table: 5\n"
     ]
    }
   ],
   "source": [
    "print('count of 1 table:', list_dfs[0].count()) # this is going to be 5 * 3 = 15 after union_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "789389e1-89e0-4921-8d02-33e04909e9ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[DataFrame[val1: string, val2: string, is_joined_main: int, is_joined_ref: int],\n",
       " DataFrame[val1: string, val2: string, is_joined_main: int, is_joined_ref: null],\n",
       " DataFrame[val1: string, val2: string, is_joined_main: null, is_joined_ref: int]]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(list_dfs)) # 3 DFs in the list\n",
    "list_dfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b491b0f1-8e71-490b-aeca-457f74314fac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count of table after 3 unions: 15\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[val1: string, val2: string, is_joined_main: int, is_joined_ref: int]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_from_union = union_all(*list_dfs).cache()\n",
    "\n",
    "# union_all(list_dfs[0], list_dfs[1], list_dfs[2]) # equivalent\n",
    "print('count of table after 3 unions:', df_from_union.count())\n",
    "df_from_union"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "075be460-e253-4f11-9bdd-87a91274b9ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+--------------+-------------+-------+\n",
      "|val1|val2|is_joined_main|is_joined_ref|is_diff|\n",
      "+----+----+--------------+-------------+-------+\n",
      "|   x|   x|             1|            1|      0|\n",
      "|   x|   y|             1|            1|      1|\n",
      "|   x|null|             1|            1|      1|\n",
      "|null|   x|             1|            1|      1|\n",
      "|null|null|             1|            1|      0|\n",
      "|   x|   x|             1|         null|      0|\n",
      "|   x|   y|             1|         null|      0|\n",
      "|   x|null|             1|         null|      0|\n",
      "|null|   x|             1|         null|      0|\n",
      "|null|null|             1|         null|      0|\n",
      "|   x|   x|          null|            1|      0|\n",
      "|   x|   y|          null|            1|      0|\n",
      "|   x|null|          null|            1|      0|\n",
      "|null|   x|          null|            1|      0|\n",
      "|null|null|          null|            1|      0|\n",
      "+----+----+--------------+-------------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# this is exactly a filter in the script for comparing tables\n",
    "dummy1, dummy2,val1,val2='is_joined_main','is_joined_ref','val1','val2'\n",
    "cond_diff = f\"\"\"case when\n",
    "                ({dummy1} is null or {dummy2} is null) \n",
    "                or\n",
    "                ({val1} is null and {val2} is null)\n",
    "                or \n",
    "                ({val1} = {val2})\n",
    "                then 0\n",
    "                else 1\n",
    "            end\"\"\"\n",
    "\n",
    "(\n",
    "    df_from_union\n",
    "    .withColumn('is_diff', F.expr(cond_diff))\n",
    "    .show(100)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a501860-a975-4ee3-9539-60bb1342053a",
   "metadata": {},
   "source": [
    "### reading from Hive"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2883d83-c154-42d2-92cb-61d9104ae00b",
   "metadata": {},
   "source": [
    "1. straight parquet files\n",
    "2. using hive query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "8333c126-8aa8-44c6-9781-056a3359d20a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.read.parquet('/Users/pyro/github/HiveHelper_on_PySpark/spark-warehouse/part_table_test1/dt_part=2022-12-15/*').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "775e8729-1a66-4076-b123-03a7f08e3ed4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|count(1)|\n",
      "+--------+\n",
      "|       1|\n",
      "+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select count(1) from default.part_table_test1 where dt_part='2022-12-15'\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc6084b6-4744-49ae-b885-046e7bf424d0",
   "metadata": {},
   "source": [
    "### writing DataFrames to Hive"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d5047f6-c752-4075-9179-69d88e2bd11c",
   "metadata": {},
   "source": [
    "needs refining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "009ea19b-7237-44b9-bb7b-579e34a6d127",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_from_union_write = df_from_union.limit(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "3cec261d-548c-4a1a-9c00-31e629686b15",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DF saved as default.test_writing_1\n"
     ]
    }
   ],
   "source": [
    "# 1\n",
    "write_table(df_from_union_write, 'test_writing_1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "1acd7154-64a5-4275-992f-d7ccb2658ef8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_from_union_write.write.mode('overwrite').saveAsTable('default.test_writing_3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "859dd438-a682-4d7a-b326-052108060204",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DF saved as default.hello_test3\n"
     ]
    }
   ],
   "source": [
    "write_table(df, 'hello_test3', partition_cols=['index', 'var1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "91377734-5047-4eb9-8d4e-e175dbacb1cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- pk1: string (nullable = true)\n",
      " |-- pk2: string (nullable = true)\n",
      " |-- var2: string (nullable = true)\n",
      " |-- dt_part: string (nullable = true)\n",
      " |-- group_part: string (nullable = true)\n",
      " |-- index: string (nullable = true)\n",
      " |-- var1: string (nullable = true)\n",
      "\n",
      "partition columns: ['index', 'var1']\n",
      "location: file:/Users/pyro/github/HiveHelper_on_PySpark/spark-warehouse/hello_test3\n",
      "6 parquet files at the location\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[pk1: string, pk2: string, var2: string, dt_part: string, group_part: string, index: string, var1: string]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "read_table('default.hello_test3', verbose=1, cnt_files=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdd0263d-7f7e-453f-b164-7b6bf9a723b6",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Modification of code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a97c0be-a9f4-417e-bb02-5598fbbdaf43",
   "metadata": {},
   "source": [
    "1. read as you like, use DFExtender to get stats\n",
    "2. use all methods from PySpark as usual (beware that PySpark methods return a DataFrame object, not DFExtender object)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "416e15c9-4728-483d-8387-432b17145b87",
   "metadata": {},
   "source": [
    "Check out official documentation!\n",
    "1. [pyspark.sql.DataFrame methods](https://spark.apache.org/docs/3.1.1/api/python/reference/api/pyspark.sql.DataFrame.html)\n",
    "2. [PySpark functions](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/functions.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "bffc22ab-b020-4305-b6a7-3c917f920c52",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = read_table('default.table1_comp', alias='main')\n",
    "df_main = DFExtender(df, pk=['pk1', 'pk2'], verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "500d2dd6-b71c-48ff-ada8-a8aa264301a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'hhop.DFExtender'>\n"
     ]
    }
   ],
   "source": [
    "print(df_main.__class__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "99d35d00-8d16-467e-9737-f6d0efa8528a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply PySpark method for DFExtender object\n",
    "df_main_filter = df_main.filter(col('pk1').isNotNull())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "4dafcedb-abb6-43a4-95b4-8438ebd58cdb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pyspark.sql.dataframe.DataFrame'>\n"
     ]
    }
   ],
   "source": [
    "print(df_main_filter.__class__) # the type of an object returns to Spark DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "90bd05ac-3f55-43d3-83a9-997710829417",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
