{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54b7f6f1-a926-4ae8-824e-dd31e6beeb50",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '/Users/pyro/github/HiveHelper_on_PySpark/hhop') \n",
    "# for running .ipynb files anywhere outside of a current dir using the module hhop\n",
    "\n",
    "from functools import reduce\n",
    "from importlib import reload\n",
    "import pandas as pd\n",
    "\n",
    "from pyspark.sql.functions import col\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.window import Window as W\n",
    "\n",
    "import shutil, os, time # working with FS\n",
    "from glob import glob\n",
    "from shutil import copy2\n",
    "from pathlib import Path\n",
    "\n",
    "import hhop # custom module\n",
    "from hhop import DFExtender, SchemaManager #main classes\n",
    "import funs\n",
    "from funs import read_table, write_table, union_all # useful functions\n",
    "from spark_init import spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22500124-12d5-4e1d-91be-e905334b3add",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(spark)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19582cd6-8bba-4576-bafc-4f751384e75b",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Creating a synth table from csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ac7ee8a-a703-44f5-86d6-db9a77346caf",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_src = spark.read.csv('./synth_data/table1.csv', header=True, sep=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaaa138e-7ef5-4008-a5ee-364cf507e99a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_src.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bf11d6f-563e-4652-add7-5c15e48ec599",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_src.write.mode('overwrite').partitionBy('dt_part', 'group_part').saveAsTable('default.part_table_test1')\n",
    "df_src.repartition(4).write.mode('overwrite').saveAsTable('default.nonpart_table_test1')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab0a2abe-01e2-413a-8f04-deaacf93b1fb",
   "metadata": {},
   "source": [
    "## Info about table"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d11616d9-de5d-4dde-8f96-ceab126beb13",
   "metadata": {},
   "source": [
    "### Reading table from Hive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77c4f3ce-0d4b-4174-925f-1b6f5025ca02",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = read_table('default.part_table_test1', verbose=True, cnt_files=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2337e3e6-678a-4394-b73b-8163d15e8786",
   "metadata": {},
   "source": [
    "You can use the next shell command to get a number of parquet files in any subdirectory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44d043c9-9144-46b7-9c03-90e4cdfa7491",
   "metadata": {},
   "outputs": [],
   "source": [
    "!hdfs dfs -ls -R file:/Users/pyro/github/HiveHelper_on_PySpark/spark-warehouse/part_table_test1/dt_part=2022-12-19 | grep '.parquet' | wc -l"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2da7742e-b7a7-4e13-9827-8e2b106a55f8",
   "metadata": {},
   "source": [
    "**Whenever you get a DF from DFExtender do not forget to either**  \n",
    "1. write to Hive using custom function (same as 2 method but with defaults) `write_table(df, table, ...)`\n",
    "2. write to Hive using native Spark methods: `df.write.mode('overwrite').saveAsTable('schema.table')`\n",
    "3. or cache DF like `df = df.cache()`\n",
    "\n",
    "\n",
    "Otherwise Spark will read sources of this DF every time and it could be very time consuming."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bdd6edd-00c2-4575-bebc-2ee432eb8234",
   "metadata": {
    "tags": []
   },
   "source": [
    "### NULL checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "764b69ff-436b-497a-92fb-21ace914e09c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DFExtender doesn't change DataFrame during initialization and returns it as is\n",
    "df_check = DFExtender(df, pk=['pk1', 'pk2'], verbose=True)\n",
    "df_check.get_info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f097bbe-6a66-425b-9582-d191785fd189",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this method returns a DF sorted by count of nulls in selected columns in descending order\n",
    "df_check_null = df_check.get_df_with_null(['var1', 'var2'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ea5bd21-e2e3-4418-acbf-f5c3415b2eac",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_check_null.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea14c1d7-303f-4b7f-8ad9-96b7bf78c06f",
   "metadata": {},
   "source": [
    "### Primary Key checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "820e8d38-2962-4e40-9424-55388a8a8753",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_check = DFExtender(df, pk=['pk1', 'pk2'], verbose=True)\n",
    "df_check.get_info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73fd79c0-1922-4faf-bd48-c5967f364ec4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_check.df_duplicates_pk.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93ccdd9c-255d-4c80-911a-9c544c826eb6",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Comparing tables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6f3b51e-7ec3-40f0-abba-388fe7fbedab",
   "metadata": {},
   "source": [
    "Sometimes you need to compare two tables based on its primary keys.    \n",
    "This method does exactly that. It\n",
    "1. joins two DFs\n",
    "2. calculates statistics from `DFExtender.get_info()`\n",
    "3. print statistics on joining two tables, errors on non-PK attributes\n",
    "4. returns DF with errors for manual analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96b27f14-79ca-407f-b0e8-e3278d420cc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_synth_sample(name):\n",
    "    (\n",
    "        spark.read.csv(f'./synth_data/{name}.csv', header=True, sep=';')\n",
    "        .write.mode('overwrite')\n",
    "        .partitionBy('dt_part', 'group_part')\n",
    "        .saveAsTable(f'default.{name}')\n",
    "    )\n",
    "    \n",
    "write_synth_sample('table1_comp')  \n",
    "write_synth_sample('table2_comp')\n",
    "\n",
    "df = read_table('default.table1_comp', alias='main')\n",
    "df_ref = read_table('default.table2_comp', alias='ref')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c20e2c3b-32f8-4880-8c6f-36b27d619a12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # For testing DFs without common columns outside of PK\n",
    "# df=df.select(['pk1', 'pk2'])\n",
    "# df_ref=df_ref.select(['pk1', 'pk2'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ffc4a6e-9cbd-4927-ba2b-e0e818e4a575",
   "metadata": {},
   "source": [
    "Instance of DFExtender is the **main DF**,   \n",
    "DF in arguments is the **reference DF**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc2060b0-65b1-4bd9-9885-034e870b5eeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_main = DFExtender(df, pk=['pk1', 'pk2'], verbose=True)\n",
    "df_main.compare_tables(df_ref)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd68b215-1932-4a6e-a7e9-8922fa9760a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_matching_errors = df_main.df_with_errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f90fdd1-ad48-47e9-a6a9-7b06bc562356",
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter for rows that are \"not in main table\"\n",
    "df_matching_errors.filter(col('is_joined_main').isNull())\\\n",
    ".count() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d94714c-8ab5-48c5-ae5b-367d8738824d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter for rows that are \"not in reference table\"\n",
    "df_matching_errors.filter(col('is_joined_ref').isNull())\\\n",
    ".count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d940e2ce-b030-4887-90a0-94b549a56d42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter for finding an exact difference in column\n",
    "df_matching_errors.filter(col('var1_is_diff') == 1).select('var1_is_diff', 'var1_main', 'var1_ref').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24551538-34b8-422c-a517-2fcf98a7784e",
   "metadata": {},
   "source": [
    "## SchemaManager"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac0dcafa-9751-414f-a08d-43b9cce4628a",
   "metadata": {},
   "source": [
    "This class provides an interface for analyzing how many tables in a schema don't have underlying folders or any data.  \n",
    "Then you can drop empty or broken tables from the selected schema."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e680e417-3e61-4615-a855-9b91fcc95e72",
   "metadata": {},
   "outputs": [],
   "source": [
    "schema_name = 'popular_schema' # our chosen schema for inspection\n",
    "spark.sql(f\"drop database if exists {schema_name} cascade\")\n",
    "spark.sql(f'create database {schema_name}')\n",
    "None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8544bc1-11f5-4734-ae7d-fef940760aeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_src_write = df_src.write.mode('overwrite')\n",
    "df_src_write.partitionBy('dt_part', 'group_part').saveAsTable(f'{schema_name}.table1')\n",
    "df_src_write.saveAsTable(f'{schema_name}.table2')\n",
    "df_src_write.saveAsTable(f'{schema_name}.table3')\n",
    "spark.sql(f\"drop view if exists {schema_name}.my_view\")\n",
    "spark.sql(f\"create view {schema_name}.my_view as select * from {schema_name}.table1\")\n",
    "None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "046ce783-1957-4964-843b-243aeff49fe0",
   "metadata": {},
   "source": [
    "We created 3 sample tables and 1 view:\n",
    "\n",
    "1. table1 has dir + data. It **won't** be deleted.  \n",
    "2. table2 has only root dir and no data. It **will** be deleted.  \n",
    "3. table3 doesn't have any dir and data. It **will** be deleted.\n",
    "4. my_view is a **view** and it is going to be **ignored**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd7a039e-d00b-4a15-ac0b-d6dd80b3edee",
   "metadata": {},
   "outputs": [],
   "source": [
    "table2_path = './spark-warehouse/popular_schema.db/table2'\n",
    "table3_path = './spark-warehouse/popular_schema.db/table3'\n",
    "\n",
    "shutil.rmtree(table2_path, ignore_errors=True)\n",
    "os.makedirs(table2_path, exist_ok=True)\n",
    "\n",
    "shutil.rmtree(table3_path, ignore_errors=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bb55ebf-3739-48d7-b587-0d4d494d9746",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(f\"show tables in {schema_name}\").show(10, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f293f40f-a96c-465d-b66a-ac90a3cc6b52",
   "metadata": {},
   "outputs": [],
   "source": [
    "popular_schema = SchemaManager('popular_schema')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cb9fad1-b0e4-44ba-b38e-7b1215458837",
   "metadata": {},
   "outputs": [],
   "source": [
    "popular_schema.find_empty_tables()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e1072c1-a9e0-4d65-9ace-07e7af96f5d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "popular_schema.dict_of_tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63446417-41ab-44e0-8b99-4d3042ffa3d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "popular_schema.drop_empty_tables()\n",
    "# errors are OK, because sometimes you need to remove folders, but data in Metastore stays the same"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f657d0b-94b2-4d22-97e8-8f028e74f8ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(f\"show tables in {schema_name}\").show(10, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "684b3ecb-9834-47bc-94c8-f40fa96547a2",
   "metadata": {},
   "source": [
    "## Extra"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27775b51-7ee1-46b0-926e-ea7865d2332e",
   "metadata": {},
   "source": [
    "### function `union_all`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60c6b869-5fe4-4551-b65a-224862a89878",
   "metadata": {},
   "source": [
    "This function allows making a union operation of any number of Spark DataFrames  \n",
    "Requirements:\n",
    "1. all DFs must have same columns\n",
    "2. If `dfs` is a list, explode it like `*dfs` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "689ee851-10e8-42be-ab36-6c7991c952e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generating list of 3 DataFrames (5 row count each)\n",
    "list_dfs = []\n",
    "values = [\n",
    "        (\"x\",\"x\"),\n",
    "        (\"x\",\"y\"),\n",
    "        (\"x\",None),\n",
    "        (None,\"x\"),\n",
    "        (None,None),\n",
    "    ]\n",
    "columns = ['val1', 'val2']\n",
    "for val1, val2 in ((1,1), (1, None), (None, 1)):\n",
    "    df_test = spark.createDataFrame(values, columns)\n",
    "    df_test = (\n",
    "        df_test\n",
    "        .withColumn('is_joined_main', F.lit(val1))\n",
    "        .withColumn('is_joined_ref', F.lit(val2))\n",
    "    )\n",
    "    list_dfs.append(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b61c4c0b-2b63-4e59-81e9-adef55120e46",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('count of 1 table:', list_dfs[0].count()) # this is going to be 5 * 3 = 15 after union_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "789389e1-89e0-4921-8d02-33e04909e9ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(list_dfs)) # 3 DFs in the list\n",
    "list_dfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b491b0f1-8e71-490b-aeca-457f74314fac",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_from_union = union_all(*list_dfs).cache()\n",
    "\n",
    "# union_all(list_dfs[0], list_dfs[1], list_dfs[2]) # equivalent\n",
    "print('count of table after 3 unions:', df_from_union.count())\n",
    "df_from_union"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "075be460-e253-4f11-9bdd-87a91274b9ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is exactly a filter in the script for comparing tables\n",
    "dummy1, dummy2,val1,val2='is_joined_main','is_joined_ref','val1','val2'\n",
    "cond_diff = f\"\"\"case when\n",
    "                ({dummy1} is null or {dummy2} is null) \n",
    "                or\n",
    "                ({val1} is null and {val2} is null)\n",
    "                or \n",
    "                ({val1} = {val2})\n",
    "                then 0\n",
    "                else 1\n",
    "            end\"\"\"\n",
    "\n",
    "(\n",
    "    df_from_union\n",
    "    .withColumn('is_diff', F.expr(cond_diff))\n",
    "    .show(100)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a501860-a975-4ee3-9539-60bb1342053a",
   "metadata": {},
   "source": [
    "### reading from Hive"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2883d83-c154-42d2-92cb-61d9104ae00b",
   "metadata": {},
   "source": [
    "1. straight parquet files\n",
    "2. using hive query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8333c126-8aa8-44c6-9781-056a3359d20a",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.read.parquet('/Users/pyro/github/HiveHelper_on_PySpark/spark-warehouse/part_table_test1/dt_part=2022-12-15/*').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "775e8729-1a66-4076-b123-03a7f08e3ed4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sql = spark.sql(\"select count(1) as cnt from default.part_table_test1 where dt_part='2022-12-15'\")\n",
    "df_sql.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc6084b6-4744-49ae-b885-046e7bf424d0",
   "metadata": {},
   "source": [
    "### writing DataFrames to Hive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "009ea19b-7237-44b9-bb7b-579e34a6d127",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_from_union_write = df_from_union.limit(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cec261d-548c-4a1a-9c00-31e629686b15",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 1\n",
    "write_table(df_from_union_write, 'test_writing_1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1acd7154-64a5-4275-992f-d7ccb2658ef8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_from_union_write.write.mode('overwrite').saveAsTable('default.test_writing_3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "859dd438-a682-4d7a-b326-052108060204",
   "metadata": {},
   "outputs": [],
   "source": [
    "write_table(df, 'hello_test3', partition_cols=['index', 'var1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91377734-5047-4eb9-8d4e-e175dbacb1cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "read_table('default.hello_test3', verbose=1, cnt_files=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdd0263d-7f7e-453f-b164-7b6bf9a723b6",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Modification of code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a97c0be-a9f4-417e-bb02-5598fbbdaf43",
   "metadata": {},
   "source": [
    "1. read as you like, use DFExtender to get stats\n",
    "2. use all methods from PySpark as usual (beware that PySpark methods return a DataFrame object, not DFExtender object)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "416e15c9-4728-483d-8387-432b17145b87",
   "metadata": {},
   "source": [
    "Check out official documentation!\n",
    "1. [pyspark.sql.DataFrame methods](https://spark.apache.org/docs/3.1.1/api/python/reference/api/pyspark.sql.DataFrame.html)\n",
    "2. [PySpark functions](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/functions.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bffc22ab-b020-4305-b6a7-3c917f920c52",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = read_table('default.table1_comp', alias='main')\n",
    "df_main = DFExtender(df, pk=['pk1', 'pk2'], verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "500d2dd6-b71c-48ff-ada8-a8aa264301a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_main.__class__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99d35d00-8d16-467e-9737-f6d0efa8528a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply PySpark method for DFExtender object\n",
    "df_main_filter = df_main.filter(col('pk1').isNotNull())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dafcedb-abb6-43a4-95b4-8438ebd58cdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_main_filter.__class__) # the type of an object returns to Spark DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2840e00d-7e0e-449b-ba4c-83d8be1e52b7",
   "metadata": {},
   "source": [
    "### Generating txt files for sending over email"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c591f532-6914-4872-b71a-eba1b12897fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "py_files = glob('./hhop/*.py')\n",
    "os.makedirs('./hhop/txt', exist_ok=True)\n",
    "\n",
    "for file in py_files:\n",
    "    filename = file.split('/')[-1]\n",
    "#     copying\n",
    "    full_path = f'./hhop/txt/{filename}'\n",
    "    copy2(file, full_path)\n",
    "#     renaming\n",
    "    p = Path(full_path)\n",
    "    p.rename(p.with_suffix('.txt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90bd05ac-3f55-43d3-83a9-997710829417",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
