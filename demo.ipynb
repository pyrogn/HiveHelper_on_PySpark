{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "54b7f6f1-a926-4ae8-824e-dd31e6beeb50",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/07/09 02:31:25 WARN Utils: Your hostname, Pavels-MacBook-Air.local resolves to a loopback address: 127.0.0.200; using 192.168.0.103 instead (on interface en0)\n",
      "23/07/09 02:31:25 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "23/07/09 02:31:25 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - hive</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://192.168.0.103:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.1.2</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>custom_app_name123</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7fbd70c75300>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '/Users/pyro/github/HiveHelper_on_PySpark/hhop') \n",
    "# for running .ipynb files anywhere outside of a current dir using the module hhop\n",
    "\n",
    "from functools import reduce\n",
    "from importlib import reload\n",
    "import pandas as pd\n",
    "\n",
    "from pyspark.sql.functions import col\n",
    "import pyspark.sql.functions as F\n",
    "import pyspark.sql.types as T\n",
    "from pyspark.sql.window import Window as W\n",
    "\n",
    "import shutil, os, time # working with FS\n",
    "from glob import glob\n",
    "from shutil import copy2\n",
    "from pathlib import Path\n",
    "\n",
    "custom_spark_params = {\n",
    "    'app_name': 'custom_app_name123',\n",
    "}\n",
    "from pass_spark_config import write_spark_config\n",
    "write_spark_config(custom_spark_params)\n",
    "\n",
    "\n",
    "from hhop import DFExtender, SchemaManager, TablePartitionDescriber #main classes\n",
    "from funs import read_table, write_table, write_read_table, union_all, deduplicate_df # useful functions\n",
    "from spark_init import spark\n",
    "from exceptions import HhopException\n",
    "display(spark)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19582cd6-8bba-4576-bafc-4f751384e75b",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Creating a synth table from csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8ac7ee8a-a703-44f5-86d6-db9a77346caf",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_src = spark.read.csv('./synth_data/table1.csv', header=True, sep=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9494f1bb-93d6-4ae3-838a-3bf4ed3482b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_src = df_src.fillna('', subset=['var1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "aaaa138e-7ef5-4008-a5ee-364cf507e99a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----+----+------+--------+----------+----------+\n",
      "|index| pk1| pk2|  var1|    var2|   dt_part|group_part|\n",
      "+-----+----+----+------+--------+----------+----------+\n",
      "|    1|key1|   1|value1|value2_1|2022-12-15|    group1|\n",
      "|    2|key1|   1|value1|value2_1|2022-12-16|    group2|\n",
      "|    3|key1|   2|value1|value2_1|2022-12-16|    group3|\n",
      "|    4|key2|   2|      |value2_1|2022-12-17|    group1|\n",
      "|    5|key2|   3|value1|value2_1|2022-12-18|    group2|\n",
      "|    6|key2|   4|value1|    null|2022-12-19|    group3|\n",
      "|    7|key2|null|value1|value2_1|2022-12-19|    group4|\n",
      "|    8|null|   4|value1|value2_1|2022-12-20|    group3|\n",
      "|    9|null|null|value1|value2_1|2022-12-20|    group7|\n",
      "+-----+----+----+------+--------+----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_src.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9bf11d6f-563e-4652-add7-5c15e48ec599",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_src.write.mode('overwrite').partitionBy('dt_part', 'group_part').saveAsTable('default.part_table_test1')\n",
    "df_src.repartition(4).write.mode('overwrite').saveAsTable('default.nonpart_table_test1')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab0a2abe-01e2-413a-8f04-deaacf93b1fb",
   "metadata": {},
   "source": [
    "## Info about table"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d11616d9-de5d-4dde-8f96-ceab126beb13",
   "metadata": {},
   "source": [
    "### Reading table from Hive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "77c4f3ce-0d4b-4174-925f-1b6f5025ca02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- index: string (nullable = true)\n",
      " |-- pk1: string (nullable = true)\n",
      " |-- pk2: string (nullable = true)\n",
      " |-- var1: string (nullable = true)\n",
      " |-- var2: string (nullable = true)\n",
      " |-- dt_part: string (nullable = true)\n",
      " |-- group_part: string (nullable = true)\n",
      "\n",
      "partition columns: ['dt_part', 'group_part']\n",
      "Running command: hdfs dfs -ls -R file:/Users/pyro/github/HiveHelper_on_PySpark/spark-warehouse/part_table_test1 | grep '.parquet' | wc -l\n",
      "0 parquet files in the specified above location\n"
     ]
    }
   ],
   "source": [
    "df = read_table('default.part_table_test1', verbose=True, cnt_files=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2337e3e6-678a-4394-b73b-8163d15e8786",
   "metadata": {},
   "source": [
    "You can use the next shell command to get a number of parquet files in any subdirectory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "44d043c9-9144-46b7-9c03-90e4cdfa7491",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       0\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -ls -R file:/Users/pyro/github/HiveHelper_on_PySpark/spark-warehouse/part_table_test1/dt_part=2022-12-19 | grep '.parquet' | wc -l"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2da7742e-b7a7-4e13-9827-8e2b106a55f8",
   "metadata": {},
   "source": [
    "**Whenever you get a DF from DFExtender do not forget to either**  \n",
    "1. write to Hive using custom function (same as 2 method but with defaults) `write_table(df, table, ...)`\n",
    "2. write to Hive using native Spark methods: `df.write.mode('overwrite').saveAsTable('schema.table')`\n",
    "3. or cache DF like `df = df.cache()`\n",
    "\n",
    "\n",
    "Otherwise Spark will read sources of this DF every time and it could be very time consuming."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bdd6edd-00c2-4575-bebc-2ee432eb8234",
   "metadata": {
    "tags": []
   },
   "source": [
    "### NULL checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "764b69ff-436b-497a-92fb-21ace914e09c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You can access DF with PK duplicates in an attribute `.df_duplicates_pk`\n",
      "\n",
      "Count all:                9\n",
      "Unique PK count:          8\n",
      "PK with duplicates:       1\n",
      "\n",
      "Null values in columns - {'column': [count NULL, share NULL]}:\n",
      "{'pk1': [2, 0.2222], 'pk2': [2, 0.2222], 'var1': [1, 0.1111], 'var2': [1, 0.1111]}\n",
      "Use method `.get_df_with_null(List[str])` to get a df with specified NULL columns\n",
      "PK column 'pk1' contains empty values, be careful!\n",
      "PK column 'pk2' contains empty values, be careful!\n"
     ]
    }
   ],
   "source": [
    "# DFExtender doesn't change DataFrame during initialization and returns it as is\n",
    "df_check = DFExtender(df, pk=['pk1', 'pk2'], verbose=True)\n",
    "df_check.get_info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2f097bbe-6a66-425b-9582-d191785fd189",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this method returns a DF sorted by count of nulls in selected columns in descending order\n",
    "df_check_null = df_check.get_df_with_null(['var1', 'var2'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0ea5bd21-e2e3-4418-acbf-f5c3415b2eac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----+---+------+--------+----------+----------+---------+\n",
      "|index| pk1|pk2|  var1|    var2|   dt_part|group_part|cnt_nulls|\n",
      "+-----+----+---+------+--------+----------+----------+---------+\n",
      "|    6|key2|  4|value1|    null|2022-12-19|    group3|        1|\n",
      "|    4|key2|  2|      |value2_1|2022-12-17|    group1|        1|\n",
      "+-----+----+---+------+--------+----------+----------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_check_null.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea14c1d7-303f-4b7f-8ad9-96b7bf78c06f",
   "metadata": {},
   "source": [
    "### Primary Key checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "820e8d38-2962-4e40-9424-55388a8a8753",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You can access DF with PK duplicates in an attribute `.df_duplicates_pk`\n",
      "\n",
      "Count all:                9\n",
      "Unique PK count:          8\n",
      "PK with duplicates:       1\n",
      "\n",
      "Null values in columns - {'column': [count NULL, share NULL]}:\n",
      "{'pk1': [2, 0.2222], 'pk2': [2, 0.2222], 'var1': [1, 0.1111], 'var2': [1, 0.1111]}\n",
      "Use method `.get_df_with_null(List[str])` to get a df with specified NULL columns\n",
      "PK column 'pk1' contains empty values, be careful!\n",
      "PK column 'pk2' contains empty values, be careful!\n"
     ]
    }
   ],
   "source": [
    "df_check = DFExtender(df, pk=['pk1', 'pk2'], verbose=True)\n",
    "df_check.get_info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "73fd79c0-1922-4faf-bd48-c5967f364ec4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----+---+------+--------+----------+----------+------+\n",
      "|index| pk1|pk2|  var1|    var2|   dt_part|group_part|cnt_pk|\n",
      "+-----+----+---+------+--------+----------+----------+------+\n",
      "|    1|key1|  1|value1|value2_1|2022-12-15|    group1|     2|\n",
      "|    2|key1|  1|value1|value2_1|2022-12-16|    group2|     2|\n",
      "+-----+----+---+------+--------+----------+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_check.df_duplicates_pk.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93ccdd9c-255d-4c80-911a-9c544c826eb6",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Comparing tables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6f3b51e-7ec3-40f0-abba-388fe7fbedab",
   "metadata": {},
   "source": [
    "Sometimes you need to compare two tables based on its primary keys.    \n",
    "This method does exactly that. It\n",
    "1. joins two DFs\n",
    "2. calculates statistics from `DFExtender.get_info()`\n",
    "3. print statistics on joining two tables, errors on non-PK attributes\n",
    "4. returns DF with errors for manual analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "96b27f14-79ca-407f-b0e8-e3278d420cc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_synth_sample(name):\n",
    "    (\n",
    "        spark.read.csv(f'./synth_data/{name}.csv', header=True, sep=';')\n",
    "        .write.mode('overwrite')\n",
    "        .partitionBy('dt_part', 'group_part')\n",
    "        .saveAsTable(f'default.{name}')\n",
    "    )\n",
    "    \n",
    "write_synth_sample('table1_comp')  \n",
    "write_synth_sample('table2_comp')\n",
    "\n",
    "df = read_table('default.table1_comp', alias='main')\n",
    "df_ref = read_table('default.table2_comp', alias='ref')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c20e2c3b-32f8-4880-8c6f-36b27d619a12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # For testing DFs without common columns outside of PK\n",
    "# df=df.select(['pk1', 'pk2'])\n",
    "# df_ref=df_ref.select(['pk1', 'pk2'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ffc4a6e-9cbd-4927-ba2b-e0e818e4a575",
   "metadata": {},
   "source": [
    "Instance of DFExtender is the **main DF**,   \n",
    "DF in arguments is the **reference DF**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "dc2060b0-65b1-4bd9-9885-034e870b5eeb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Main DF\n",
      "Count all:                6\n",
      "Unique PK count:          6\n",
      "PK with duplicates:       0\n",
      "\n",
      "Reference DF\n",
      "Count all:                6\n",
      "Unique PK count:          6\n",
      "PK with duplicates:       0\n",
      "\n",
      "Errors in columns - {'column': [count is_error, share is_error]}\n",
      "{'var1': [2, 0.4], 'group_part': [2, 0.4], 'var2': [1, 0.2]}\n",
      "\n",
      "Count stats of matching main and reference tables:\n",
      "not in main table:        1\n",
      "not in reference table:   1\n",
      "correct matching:         5\n",
      "\n",
      "Use DF in attribute `.df_with_errors` for further analysis\n",
      "You can find alternative order of columns in attr .columns_diff_reordered_all\n"
     ]
    }
   ],
   "source": [
    "df_main = DFExtender(df, pk=['pk1', 'pk2'], verbose=True)\n",
    "df_main.compare_tables(df_ref)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73bc6206-69ab-47a3-9070-7b1976a61df5",
   "metadata": {},
   "source": [
    "**You can get results in native Python data types**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5f0e7340-60ca-4263-b56e-6bf5b6443a4b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'var1': [2, 0.4], 'group_part': [2, 0.4], 'var2': [1, 0.2]}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_main.dict_cols_with_errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "819b69bd-2098-4bd4-8272-7dc957c68020",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 1, 5]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_main.matching_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bd68b215-1932-4a6e-a7e9-8922fa9760a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_matching_errors = df_main.df_with_errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4f90fdd1-ad48-47e9-a6a9-7b06bc562356",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# filter for rows that are \"not in main table\"\n",
    "df_matching_errors.filter(col('is_joined_main').isNull())\\\n",
    ".count() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a3e8fdd4-240c-4e15-9be9-d7549d3a1eb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---+--------------+-------------+---------+--------+\n",
      "| pk1|pk2|is_joined_main|is_joined_ref|var1_main|var1_ref|\n",
      "+----+---+--------------+-------------+---------+--------+\n",
      "|key2|  5|          null|            1|     null|  value1|\n",
      "+----+---+--------------+-------------+---------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "(\n",
    "    df_matching_errors\n",
    "    .filter(col('is_joined_main').isNull())\n",
    "    .select('pk1', 'pk2', 'is_joined_main', 'is_joined_ref', 'var1_main', 'var1_ref')\n",
    ").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7d94714c-8ab5-48c5-ae5b-367d8738824d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# filter for rows that are \"not in reference table\"\n",
    "df_matching_errors.filter(col('is_joined_ref').isNull())\\\n",
    ".count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d940e2ce-b030-4887-90a0-94b549a56d42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+---------+--------------+\n",
      "|var1_is_diff|var1_main|      var1_ref|\n",
      "+------------+---------+--------------+\n",
      "|           1|   value1|       value19|\n",
      "|           1|     null|value_not_null|\n",
      "+------------+---------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# filter for finding an exact difference in column\n",
    "df_matching_errors.filter(col('var1_is_diff') == 1).select('var1_is_diff', 'var1_main', 'var1_ref').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b55fe01-48d1-403c-9c17-0d12ab002e1b",
   "metadata": {},
   "source": [
    "### Alternative order of columns in `df_with_errors`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "634e9060-63ab-477a-b98a-6a38a8d22315",
   "metadata": {},
   "outputs": [],
   "source": [
    "alt_order_cols = df_main.columns_diff_reordered_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1e3dd35c-3ec5-4506-9741-dd0a36fc4842",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---+--------------+-------------+------------+-----------+---------------+----------+---------+-------------+---------+--------+------------+---------+--------------+------------+---------------+--------------+------------------+\n",
      "| pk1|pk2|is_joined_main|is_joined_ref|dt_part_main|dt_part_ref|dt_part_is_diff|index_main|index_ref|index_is_diff|var2_main|var2_ref|var2_is_diff|var1_main|      var1_ref|var1_is_diff|group_part_main|group_part_ref|group_part_is_diff|\n",
      "+----+---+--------------+-------------+------------+-----------+---------------+----------+---------+-------------+---------+--------+------------+---------+--------------+------------+---------------+--------------+------------------+\n",
      "|key1|  1|             1|            1|  2022-12-15| 2022-12-15|              0|         1|        1|            0| value2_1|value2_1|           0|   value1|       value19|           1|         group2|        group7|                 1|\n",
      "|key2|  1|             1|            1|  2022-12-17| 2022-12-17|              0|         4|        4|            0| value2_1|value2_1|           0|     null|value_not_null|           1|         group1|       group10|                 1|\n",
      "+----+---+--------------+-------------+------------+-----------+---------------+----------+---------+-------------+---------+--------+------------+---------+--------------+------------+---------------+--------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "(\n",
    "    df_matching_errors\n",
    "    .select(*alt_order_cols)\n",
    "    .filter(col('var1_is_diff') == 1)\n",
    ").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09fd4e72-7317-4dd7-894f-03ad6278548d",
   "metadata": {},
   "source": [
    "## TablePartitionDescriber\n",
    "The class helps to get partitions of partitioned Hive table\n",
    "    in a readable and ready-to-use format\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fab69c8b-7e28-48da-ab20-477734d2c4b1",
   "metadata": {},
   "source": [
    "How **default format** looks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "36bc44f2-e38a-4ee4-b71c-761f48cf203a",
   "metadata": {},
   "outputs": [],
   "source": [
    "table_to_analyze_partitions = 'default.part_table_test1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c6fd4a0d-816c-4a2a-ae37-c1c3512dc528",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------------+\n",
      "|partition                           |\n",
      "+------------------------------------+\n",
      "|dt_part=2022-12-15/group_part=group1|\n",
      "|dt_part=2022-12-16/group_part=group2|\n",
      "|dt_part=2022-12-16/group_part=group3|\n",
      "|dt_part=2022-12-17/group_part=group1|\n",
      "|dt_part=2022-12-18/group_part=group2|\n",
      "|dt_part=2022-12-19/group_part=group3|\n",
      "|dt_part=2022-12-19/group_part=group4|\n",
      "|dt_part=2022-12-20/group_part=group3|\n",
      "|dt_part=2022-12-20/group_part=group7|\n",
      "+------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(f\"show partitions {table_to_analyze_partitions}\").show(10, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdb1390f-5272-48b5-9b55-4e6231e6728d",
   "metadata": {},
   "source": [
    "How you can get partitions from **this class**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d73e5ebc-0c9c-4ade-9228-c7a416dabd8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "table_partitions = TablePartitionDescriber('default.part_table_test1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "50120828-1b7c-4dc5-be29-8faca5b25d17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+\n",
      "|dt_part   |group_part|\n",
      "+----------+----------+\n",
      "|2022-12-15|group1    |\n",
      "|2022-12-16|group2    |\n",
      "|2022-12-16|group3    |\n",
      "|2022-12-17|group1    |\n",
      "|2022-12-18|group2    |\n",
      "|2022-12-19|group3    |\n",
      "|2022-12-19|group4    |\n",
      "|2022-12-20|group3    |\n",
      "|2022-12-20|group7    |\n",
      "+----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "table_partitions_got = table_partitions.get_partitions_parsed()\n",
    "table_partitions_got.show(100, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f0baa97-d571-4f4f-ac2f-def4de884eb3",
   "metadata": {},
   "source": [
    "You can find max value from partitions in a particular column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "51652c7d-2a0a-4405-9a1a-04de8703aba7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2022-12-20'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_dt = table_partitions.get_max_value_from_partitions('dt_part')\n",
    "max_dt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c38453d6-4ce2-428b-862c-22989c9b5797",
   "metadata": {},
   "source": [
    "And apply prefilter to other partitioned columns in case you need to. It is just a shortcut."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "993095a3-4e40-4608-83ff-a1532457611d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2022-12-17'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prefilter = col('group_part') == 'group1'\n",
    "max_dt_group = table_partitions.get_max_value_from_partitions('dt_part', prefilter=prefilter)\n",
    "max_dt_group"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24551538-34b8-422c-a517-2fcf98a7784e",
   "metadata": {},
   "source": [
    "## SchemaManager"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac0dcafa-9751-414f-a08d-43b9cce4628a",
   "metadata": {},
   "source": [
    "This class provides an interface for analyzing how many tables in a schema don't have underlying folders or any data.  \n",
    "Then you can drop empty or broken tables from the selected schema."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e680e417-3e61-4615-a855-9b91fcc95e72",
   "metadata": {},
   "outputs": [],
   "source": [
    "schema_name = 'popular_schema' # our chosen schema for inspection\n",
    "spark.sql(f\"drop database if exists {schema_name} cascade\")\n",
    "spark.sql(f'create database {schema_name}')\n",
    "None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d8544bc1-11f5-4734-ae7d-fef940760aeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_src_write = df_src.write.mode('overwrite')\n",
    "df_src_write.partitionBy('dt_part', 'group_part').saveAsTable(f'{schema_name}.table1')\n",
    "df_src_write.saveAsTable(f'{schema_name}.table2')\n",
    "df_src_write.saveAsTable(f'{schema_name}.table3')\n",
    "spark.sql(f\"drop view if exists {schema_name}.my_view\")\n",
    "spark.sql(f\"create view {schema_name}.my_view as select * from {schema_name}.table1\")\n",
    "None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "046ce783-1957-4964-843b-243aeff49fe0",
   "metadata": {},
   "source": [
    "We created 3 sample tables and 1 view:\n",
    "\n",
    "1. table1 has dir + data. It **won't** be deleted.  \n",
    "2. table2 has only root dir and no data. It **will** be deleted.  \n",
    "3. table3 doesn't have any dir and data. It **will** be deleted.\n",
    "4. my_view is a **view** and it is going to be **ignored**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "dd7a039e-d00b-4a15-ac0b-d6dd80b3edee",
   "metadata": {},
   "outputs": [],
   "source": [
    "table2_path = './spark-warehouse/popular_schema.db/table2'\n",
    "table3_path = './spark-warehouse/popular_schema.db/table3'\n",
    "\n",
    "shutil.rmtree(table2_path, ignore_errors=True)\n",
    "os.makedirs(table2_path, exist_ok=True)\n",
    "\n",
    "shutil.rmtree(table3_path, ignore_errors=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "1bb55ebf-3739-48d7-b587-0d4d494d9746",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+---------+-----------+\n",
      "|database      |tableName|isTemporary|\n",
      "+--------------+---------+-----------+\n",
      "|popular_schema|my_view  |false      |\n",
      "|popular_schema|table1   |false      |\n",
      "|popular_schema|table2   |false      |\n",
      "|popular_schema|table3   |false      |\n",
      "+--------------+---------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(f\"show tables in {schema_name}\").show(10, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f293f40f-a96c-465d-b66a-ac90a3cc6b52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 tables in popular_schema\n",
      "run find_empty_tables() on instance to find empty tables in popular_schema\n"
     ]
    }
   ],
   "source": [
    "popular_schema = SchemaManager('popular_schema')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "4cb9fad1-b0e4-44ba-b38e-7b1215458837",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 tables going to be dropped out of 3 (66.67%)Data about tables is stored in an attribute '.dict_of_tables':\n",
      "1 - has data, 0 - doesn't and going to be deleted\n",
      "\n",
      "run drop_empty_tables() on instance to drop empty tables in popular_schema\n"
     ]
    }
   ],
   "source": [
    "popular_schema.find_empty_tables()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "7e1072c1-a9e0-4d65-9ace-07e7af96f5d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'table2': 0, 'table3': 0, 'table1': 1}"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "popular_schema.dict_of_tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "63446417-41ab-44e0-8b99-4d3042ffa3d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/07/09 02:31:49 ERROR FileUtils: Failed to delete file:/Users/pyro/github/HiveHelper_on_PySpark/spark-warehouse/popular_schema.db/table3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After dropping tables there are 1 tables in popular_schema\n"
     ]
    }
   ],
   "source": [
    "popular_schema.drop_empty_tables()\n",
    "# errors are OK, because sometimes you need to remove folders, but data in Metastore stays the same"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "7f657d0b-94b2-4d22-97e8-8f028e74f8ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+---------+-----------+\n",
      "|database      |tableName|isTemporary|\n",
      "+--------------+---------+-----------+\n",
      "|popular_schema|my_view  |false      |\n",
      "|popular_schema|table1   |false      |\n",
      "+--------------+---------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(f\"show tables in {schema_name}\").show(10, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "684b3ecb-9834-47bc-94c8-f40fa96547a2",
   "metadata": {},
   "source": [
    "## Extra"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27775b51-7ee1-46b0-926e-ea7865d2332e",
   "metadata": {},
   "source": [
    "### function `union_all`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60c6b869-5fe4-4551-b65a-224862a89878",
   "metadata": {},
   "source": [
    "This function allows making a union operation of any number of Spark DataFrames  \n",
    "Requirements:\n",
    "1. all DFs must have same columns\n",
    "2. If `dfs` is a list, explode it like `*dfs` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "689ee851-10e8-42be-ab36-6c7991c952e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generating list of 3 DataFrames (5 row count each)\n",
    "list_dfs = []\n",
    "values = [\n",
    "        (\"x\",\"x\"),\n",
    "        (\"x\",\"y\"),\n",
    "        (\"x\",None),\n",
    "        (None,\"x\"),\n",
    "        (None,None),\n",
    "    ]\n",
    "columns = ['val1', 'val2']\n",
    "for val1, val2 in ((1,1), (1, None), (None, 1)):\n",
    "    df_test = spark.createDataFrame(values, columns)\n",
    "    df_test = (\n",
    "        df_test\n",
    "        .withColumn('is_joined_main', F.lit(val1))\n",
    "        .withColumn('is_joined_ref', F.lit(val2))\n",
    "    )\n",
    "    list_dfs.append(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "b61c4c0b-2b63-4e59-81e9-adef55120e46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count of 1 table: 5\n"
     ]
    }
   ],
   "source": [
    "print('count of 1 table:', list_dfs[0].count()) # this is going to be 5 * 3 = 15 after union_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "789389e1-89e0-4921-8d02-33e04909e9ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[DataFrame[val1: string, val2: string, is_joined_main: int, is_joined_ref: int],\n",
       " DataFrame[val1: string, val2: string, is_joined_main: int, is_joined_ref: null],\n",
       " DataFrame[val1: string, val2: string, is_joined_main: null, is_joined_ref: int]]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(list_dfs)) # 3 DFs in the list\n",
    "list_dfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "b491b0f1-8e71-490b-aeca-457f74314fac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count of table after 3 unions: 15\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[val1: string, val2: string, is_joined_main: int, is_joined_ref: int]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_from_union = union_all(*list_dfs).cache()\n",
    "\n",
    "# union_all(list_dfs[0], list_dfs[1], list_dfs[2]) # equivalent\n",
    "print('count of table after 3 unions:', df_from_union.count())\n",
    "df_from_union"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "075be460-e253-4f11-9bdd-87a91274b9ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+--------------+-------------+-------+\n",
      "|val1|val2|is_joined_main|is_joined_ref|is_diff|\n",
      "+----+----+--------------+-------------+-------+\n",
      "|   x|   x|             1|            1|      0|\n",
      "|   x|   y|             1|            1|      1|\n",
      "|   x|null|             1|            1|      1|\n",
      "|null|   x|             1|            1|      1|\n",
      "|null|null|             1|            1|      0|\n",
      "|   x|   x|             1|         null|      0|\n",
      "|   x|   y|             1|         null|      0|\n",
      "|   x|null|             1|         null|      0|\n",
      "|null|   x|             1|         null|      0|\n",
      "|null|null|             1|         null|      0|\n",
      "|   x|   x|          null|            1|      0|\n",
      "|   x|   y|          null|            1|      0|\n",
      "|   x|null|          null|            1|      0|\n",
      "|null|   x|          null|            1|      0|\n",
      "|null|null|          null|            1|      0|\n",
      "+----+----+--------------+-------------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# this is exactly a filter in the script for comparing tables\n",
    "dummy1, dummy2,val1,val2='is_joined_main','is_joined_ref','val1','val2'\n",
    "cond_diff = f\"\"\"case when\n",
    "                ({dummy1} is null or {dummy2} is null) \n",
    "                or\n",
    "                ({val1} is null and {val2} is null)\n",
    "                or \n",
    "                ({val1} = {val2})\n",
    "                then 0\n",
    "                else 1\n",
    "            end\"\"\"\n",
    "\n",
    "(\n",
    "    df_from_union\n",
    "    .withColumn('is_diff', F.expr(cond_diff))\n",
    "    .show(100)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a501860-a975-4ee3-9539-60bb1342053a",
   "metadata": {},
   "source": [
    "### reading from Hive"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2883d83-c154-42d2-92cb-61d9104ae00b",
   "metadata": {},
   "source": [
    "1. straight parquet files\n",
    "2. using hive query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "8333c126-8aa8-44c6-9781-056a3359d20a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.read.parquet('/Users/pyro/github/HiveHelper_on_PySpark/spark-warehouse/part_table_test1/dt_part=2022-12-15/*').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "775e8729-1a66-4076-b123-03a7f08e3ed4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+\n",
      "|cnt|\n",
      "+---+\n",
      "|  1|\n",
      "+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_sql = spark.sql(\"select count(1) as cnt from default.part_table_test1 where dt_part='2022-12-15'\")\n",
    "df_sql.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc6084b6-4744-49ae-b885-046e7bf424d0",
   "metadata": {},
   "source": [
    "### writing DataFrames to Hive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "1acd7154-64a5-4275-992f-d7ccb2658ef8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1\n",
    "df.coalesce(1).write.partitionBy(['index', 'var1']).mode('overwrite').saveAsTable('default.test_writing_1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "859dd438-a682-4d7a-b326-052108060204",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DF saved as default.test_writing_2\n"
     ]
    }
   ],
   "source": [
    "# 2 (same as 1)\n",
    "# but everything is optional except DF and name of the table\n",
    "write_table(\n",
    "    df.coalesce(1), 'test_writing_2', \n",
    "    schema='default', \n",
    "    partition_cols=['index', 'var1'], \n",
    "    mode='overwrite', \n",
    "    format_files='parquet'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "91377734-5047-4eb9-8d4e-e175dbacb1cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- pk1: string (nullable = true)\n",
      " |-- pk2: string (nullable = true)\n",
      " |-- var2: string (nullable = true)\n",
      " |-- dt_part: string (nullable = true)\n",
      " |-- group_part: string (nullable = true)\n",
      " |-- index: string (nullable = true)\n",
      " |-- var1: string (nullable = true)\n",
      "\n",
      "partition columns: ['index', 'var1']\n",
      "Running command: hdfs dfs -ls -R file:/Users/pyro/github/HiveHelper_on_PySpark/spark-warehouse/test_writing_1 | grep '.parquet' | wc -l\n",
      "0 parquet files in the specified above location\n"
     ]
    }
   ],
   "source": [
    "read_table('default.test_writing_1', verbose=1, cnt_files=True)\n",
    "None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5980e6fe-dc15-4f88-b8dc-33fdde319e0d",
   "metadata": {},
   "source": [
    "### Deduplication of DF using `deduplicate_df`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "a1cbad21-262d-41b8-b34f-f488ddb90047",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----+---+------+--------+----------+----------+\n",
      "|index| pk1|pk2|  var1|    var2|   dt_part|group_part|\n",
      "+-----+----+---+------+--------+----------+----------+\n",
      "|    1|key1|  1|value1|value2_1|2022-12-15|    group2|\n",
      "|    2|key1|  2|value1|value2_1|2022-12-16|    group2|\n",
      "|    3|key1|  3|value1|value2_1|2022-12-16|    group3|\n",
      "|    5|key2|  2|value1|value2_1|2022-12-18|    group2|\n",
      "|    4|key2|  1|  null|value2_1|2022-12-17|    group1|\n",
      "|    6|key2|  3|value1|    null|2022-12-20|    group3|\n",
      "+-----+----+---+------+--------+----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "fdbae326-ee35-4f78-9feb-9f7b2493aa0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dedup = deduplicate_df(df, pk=['pk1'], order_by_cols=[col('dt_part').desc(), col('group_part')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "93e8164a-cd9d-48f1-bd22-b46dd2914012",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----+---+------+--------+----------+----------+\n",
      "|index| pk1|pk2|  var1|    var2|   dt_part|group_part|\n",
      "+-----+----+---+------+--------+----------+----------+\n",
      "|    2|key1|  2|value1|value2_1|2022-12-16|    group2|\n",
      "|    6|key2|  3|value1|    null|2022-12-20|    group3|\n",
      "+-----+----+---+------+--------+----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_dedup.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dd86419-a060-4664-b483-a39eecc0db38",
   "metadata": {},
   "source": [
    "### Making checkpoints `read_write_table`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "efbfd5a1-0770-4248-add9-eb9587dbb0a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"create database if not exists test_checkpoint\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "19f99732-c202-4448-8094-ebb43837e268",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DF saved as test_checkpoint.table_name12\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_dedup_cp = write_read_table(df_dedup, 'table_name12', schema='test_checkpoint', verbose=1)\n",
    "df_dedup_cp.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdd0263d-7f7e-453f-b164-7b6bf9a723b6",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Modification of code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a97c0be-a9f4-417e-bb02-5598fbbdaf43",
   "metadata": {},
   "source": [
    "1. read as you like, use DFExtender to get stats\n",
    "2. use all methods from PySpark as usual (beware that PySpark methods return a DataFrame object, not DFExtender object)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "416e15c9-4728-483d-8387-432b17145b87",
   "metadata": {},
   "source": [
    "Check out official documentation!\n",
    "1. [pyspark.sql.DataFrame methods](https://spark.apache.org/docs/3.1.1/api/python/reference/api/pyspark.sql.DataFrame.html)\n",
    "2. [PySpark functions](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/functions.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "bffc22ab-b020-4305-b6a7-3c917f920c52",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = read_table('default.table1_comp', alias='main')\n",
    "df_main = DFExtender(df, pk=['pk1', 'pk2'], verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "500d2dd6-b71c-48ff-ada8-a8aa264301a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'hhop.DFExtender'>\n"
     ]
    }
   ],
   "source": [
    "print(df_main.__class__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "99d35d00-8d16-467e-9737-f6d0efa8528a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply PySpark method for DFExtender object\n",
    "df_main_filter = df_main.filter(col('pk1').isNotNull())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "4dafcedb-abb6-43a4-95b4-8438ebd58cdb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pyspark.sql.dataframe.DataFrame'>\n"
     ]
    }
   ],
   "source": [
    "print(df_main_filter.__class__) # the type of an object returns to Spark DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2840e00d-7e0e-449b-ba4c-83d8be1e52b7",
   "metadata": {},
   "source": [
    "### Generating txt files for sending by email"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "c591f532-6914-4872-b71a-eba1b12897fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "py_files = glob('./hhop/*.py')\n",
    "os.makedirs('./hhop/txt', exist_ok=True)\n",
    "\n",
    "for file in py_files:\n",
    "    filename = file.split('/')[-1]\n",
    "#     copying\n",
    "    full_path = f'./hhop/txt/{filename}'\n",
    "    copy2(file, full_path)\n",
    "#     renaming\n",
    "    p = Path(full_path)\n",
    "    p.rename(p.with_suffix('.txt'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c58c4e8d-a887-4fdf-9090-d5b4d85d9424",
   "metadata": {},
   "source": [
    "## Some Tests"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5154d974-195c-4dac-96fb-ae180a1533da",
   "metadata": {},
   "source": [
    "Another option would be writing tests inside modules using mocks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dd770f5-3b37-49f1-8e59-1521b9787d29",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "ecc767d7-5525-434b-b212-e26982cd8920",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "error is catched successfully\n",
      "{'some_column_not_existed'} are not in columns of provided DF\n"
     ]
    }
   ],
   "source": [
    "try: \n",
    "    write_table(df, table='table_name1' ,schema='test_hhop', partition_cols=['some_column_not_existed'])\n",
    "except HhopException as e:\n",
    "    print('error is catched successfully\\n', e, sep='')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f10a42e-1a19-4250-9ef5-a1f9aaef6de9",
   "metadata": {},
   "source": [
    "### DFExtender"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "c99e8519-4e4d-4cd6-9e06-ea4e1c8fb579",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "error is catched successfully\n",
      "columns {'some_column_not_existed'} are not present in provided columns: ['index', 'pk1', 'pk2', 'var1', 'var2', 'dt_part', 'group_part']\n"
     ]
    }
   ],
   "source": [
    "try: \n",
    "    DFExtender(df, pk=['some_column_not_existed'])\n",
    "except HhopException as e:\n",
    "    print('error is catched successfully\\n', e, sep='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "c50c89ca-b888-4534-ac9c-9d117964d92b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count all:                6\n",
      "Unique PK count:          6\n",
      "PK with duplicates:       0\n",
      "\n",
      "Null values in columns - {'column': [count NULL, share NULL]}:\n",
      "{'var1': [1, 0.1667], 'var2': [1, 0.1667]}\n"
     ]
    }
   ],
   "source": [
    "DFExtender(df, pk=df.columns).get_info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "c0b4d99b-ee65-4629-a4c0-1f116aa1f5e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_check1 = DFExtender(df, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "83e69a62-e49f-492d-b198-e65b04faaa49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Null values in columns - {'column': [count NULL, share NULL]}:\n",
      "{'var1': [1, 0.1667], 'var2': [1, 0.1667]}\n",
      "Use method `.get_df_with_null(List[str])` to get a df with specified NULL columns\n"
     ]
    }
   ],
   "source": [
    "df_check1.get_info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "bf06088f-9589-49ee-bdaa-3ffd1c1e8b19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No NULL values found in provided [], using all: dict_keys(['var1', 'var2'])\n",
      "+-----+----+---+------+--------+----------+----------+---------+\n",
      "|index| pk1|pk2|  var1|    var2|   dt_part|group_part|cnt_nulls|\n",
      "+-----+----+---+------+--------+----------+----------+---------+\n",
      "|    4|key2|  1|  null|value2_1|2022-12-17|    group1|        1|\n",
      "|    6|key2|  3|value1|    null|2022-12-20|    group3|        1|\n",
      "+-----+----+---+------+--------+----------+----------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_check1.get_df_with_null([]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "5fd06abe-9c82-426c-9837-015943150039",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "error is catched successfully\n",
      "columns {'dt_part_unknown'} are not present in provided columns: ['index', 'pk1', 'pk2', 'var1', 'var2', 'dt_part', 'group_part']\n"
     ]
    }
   ],
   "source": [
    "try: \n",
    "    df_check1.get_df_with_null(['dt_part_unknown']).show()\n",
    "except HhopException as e:\n",
    "    print('error is catched successfully\\n', e, sep='')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d539d7e1-85ad-4574-81cd-477ebe53cbf1",
   "metadata": {},
   "source": [
    "`get_info` method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "3189f058-372f-4374-9fd3-5802e23907d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count all:                6\n",
      "Unique PK count:          6\n",
      "PK with duplicates:       0\n",
      "\n",
      "Null values in columns - {'column': [count NULL, share NULL]}:\n",
      "{'var1': [1, 0.1667], 'var2': [1, 0.1667]}\n",
      "Use method `.get_df_with_null(List[str])` to get a df with specified NULL columns\n"
     ]
    }
   ],
   "source": [
    "df_check = DFExtender(df, pk=['pk1', 'pk2'], verbose=True)\n",
    "df_check.get_info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "ee0fdcef-644d-4890-a980-427c2e120357",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Null values in columns - {'column': [count NULL, share NULL]}:\n",
      "{'var1': [1, 0.1667], 'var2': [1, 0.1667]}\n",
      "Use method `.get_df_with_null(List[str])` to get a df with specified NULL columns\n"
     ]
    }
   ],
   "source": [
    "df_check = DFExtender(df, pk=['pk1', 'pk2'], verbose=True)\n",
    "df_check.get_info(pk_stats=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "3abe8e14-f84d-4308-86a4-8837696d05dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count all:                6\n",
      "Unique PK count:          6\n",
      "PK with duplicates:       0\n"
     ]
    }
   ],
   "source": [
    "df_check = DFExtender(df, pk=['pk1', 'pk2'], verbose=True)\n",
    "df_check.get_info(null_stats=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "44349226-b385-464b-a48a-33ca75e02808",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_check = DFExtender(df, pk=['pk1', 'pk2'], verbose=True)\n",
    "df_check.get_info(null_stats=False, pk_stats=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0960e893-1fe7-4fc1-8005-8c36e1043989",
   "metadata": {},
   "source": [
    "Compare tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "92ac58d3-4fca-4722-828b-93a1d36ccb63",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = df.drop('pk2').withColumn('pk2', F.lit('unknown_value'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "6434b080-2a32-44e7-92a9-ec1802e169c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_comp1 = DFExtender(df, pk=['pk1', 'pk2'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "2347d258-e009-4297-9ecb-f09ac7793ae8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Main DF\n",
      "Count all:                6\n",
      "Unique PK count:          6\n",
      "PK with duplicates:       0\n",
      "\n",
      "Reference DF\n",
      "Count all:                6\n",
      "Unique PK count:          2\n",
      "PK with duplicates:       2\n",
      "\n",
      "There are no errors in non PK columns\n",
      "\n",
      "Count stats of matching main and reference tables:\n",
      "not in main table:        6\n",
      "not in reference table:   6\n",
      "correct matching:         0\n"
     ]
    }
   ],
   "source": [
    "df_comp1.compare_tables(df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "5c75f9fc-2a3d-4526-94a8-d52aac6f8fcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_comp2 = DFExtender(df, pk=df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "a5e994af-23a5-4ff9-9758-e4ebefbf37b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Main DF\n",
      "Count all:                6\n",
      "Unique PK count:          6\n",
      "PK with duplicates:       0\n",
      "\n",
      "Reference DF\n",
      "Count all:                6\n",
      "Unique PK count:          6\n",
      "PK with duplicates:       0\n",
      "\n",
      "There are no common columns outside of PK\n",
      "\n",
      "Count stats of matching main and reference tables:\n",
      "not in main table:        6\n",
      "not in reference table:   6\n",
      "correct matching:         0\n"
     ]
    }
   ],
   "source": [
    "df_comp2.compare_tables(df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "6e2524b7-695c-42b6-a3a8-d9aaba9d1379",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataDictionary1 = {\n",
    "    'pk': ['key1', 'key2', 'key3', 'key4'],\n",
    "    'var1': [0.035, 1.1368, 0.41, None],\n",
    "    'var2': [1,2,3,4]\n",
    "}\n",
    "\n",
    "dataDictionary2 = {\n",
    "    'pk': ['key1', 'key2', 'key3', 'key4'],\n",
    "    'var1': [0.03, 1.1361, 0.401, None],\n",
    "    'var2': [1,2,3,4]\n",
    "}\n",
    "\n",
    "\n",
    "df1_round = spark.createDataFrame(data=pd.DataFrame(dataDictionary1))\n",
    "df2_round = spark.createDataFrame(data=pd.DataFrame(dataDictionary2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "e7262afe-1c5f-413f-8260-7133652cf80b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1_round_check = DFExtender(df1_round, pk=['pk'], verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "b5148e0e-d653-4ee9-b8df-f6d82ff58cef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Main DF\n",
      "Count all:                4\n",
      "Unique PK count:          4\n",
      "PK with duplicates:       0\n",
      "\n",
      "Reference DF\n",
      "Count all:                4\n",
      "Unique PK count:          4\n",
      "PK with duplicates:       0\n",
      "\n",
      "Errors in columns - {'column': [count is_error, share is_error]}\n",
      "{'var1': [2, 0.5]}\n",
      "\n",
      "Count stats of matching main and reference tables:\n",
      "not in main table:        0\n",
      "not in reference table:   0\n",
      "correct matching:         4\n",
      "\n",
      "Use DF in attribute `.df_with_errors` for further analysis\n",
      "You can find alternative order of columns in attr .columns_diff_reordered_all\n"
     ]
    }
   ],
   "source": [
    "df1_round_check.compare_tables(df2_round)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "ad5bca4a-9998-43f7-94f3-8521dca907f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+--------------+-------------+---------+---------+--------+--------+------------+------------+----------+\n",
      "|  pk|is_joined_main|is_joined_ref|var2_main|var1_main|var2_ref|var1_ref|var2_is_diff|var1_is_diff|sum_errors|\n",
      "+----+--------------+-------------+---------+---------+--------+--------+------------+------------+----------+\n",
      "|key3|             1|            1|        3|     0.41|       3|     0.4|           0|           1|         1|\n",
      "|key1|             1|            1|        1|     0.04|       1|    0.03|           0|           1|         1|\n",
      "+----+--------------+-------------+---------+---------+--------+--------+------------+------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1_round_check.df_with_errors.filter('var1_is_diff = 1').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "e685e0cc-7d6b-4740-a12f-e25590fa9fa3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+--------------+-------------+---------+--------+------------+---------+--------+------------+\n",
      "|  pk|is_joined_main|is_joined_ref|var2_main|var2_ref|var2_is_diff|var1_main|var1_ref|var1_is_diff|\n",
      "+----+--------------+-------------+---------+--------+------------+---------+--------+------------+\n",
      "|key3|             1|            1|        3|       3|           0|     0.41|     0.4|           1|\n",
      "|key1|             1|            1|        1|       1|           0|     0.04|    0.03|           1|\n",
      "+----+--------------+-------------+---------+--------+------------+---------+--------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "alternative_order_columns = df1_round_check.columns_diff_reordered_all\n",
    "df1_round_check.df_with_errors[alternative_order_columns].filter('var1_is_diff = 1').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "90bd05ac-3f55-43d3-83a9-997710829417",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
